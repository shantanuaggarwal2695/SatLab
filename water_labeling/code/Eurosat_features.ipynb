{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72df4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from pyspark.sql.functions import col, split, expr\n",
    "from pyspark.sql.functions import udf, lit, collect_list, collect_set\n",
    "import findspark\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from pyspark.sql.functions import col, split, expr\n",
    "from pyspark.sql.functions import udf, lit, explode\n",
    "from pyspark import StorageLevel\n",
    "import seaborn as sns\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.spatialOperator import KNNQuery\n",
    "from shapely.geometry import Point\n",
    "import pyspark.sql.functions as f\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import LongType, BooleanType\n",
    "from pyspark.sql.types import IntegerType, DoubleType, ArrayType\n",
    "from pyspark.sql.functions import regexp_replace,pandas_udf,PandasUDFType\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from pyspark.sql.functions import col, split, expr\n",
    "from pyspark.sql.functions import udf, lit, collect_list, collect_set\n",
    "import findspark\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from pyspark.sql.functions import col, split, expr\n",
    "from pyspark.sql.functions import udf, lit, explode\n",
    "from pyspark import StorageLevel\n",
    "import seaborn as sns\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.spatialOperator import KNNQuery\n",
    "from shapely.geometry import Point\n",
    "import pyspark.sql.functions as f\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from cv2 import cv2\n",
    "from skimage.feature import *\n",
    "from skimage.feature import greycoprops as gc\n",
    "from pyspark.ml.feature import QuantileDiscretizer, Bucketizer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e02db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/27 17:17:59 WARN Utils: Your hostname, en4119508l resolves to a loopback address: 127.0.1.1; using 10.218.106.130 instead (on interface enp0s25)\n",
      "22/04/27 17:17:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/04/27 17:18:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/27 17:18:00 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "22/04/27 17:18:01 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [file:///hdd2/shantanuCodeData/lib/spark-logs]. Please check your configured local directories.\n",
      "22/04/27 17:18:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"spark://EN4119508L.cidse.dhcp.asu.edu:7077\").\\\n",
    "    appName(\"euro_sat\").\\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "    config(\"spark.driver.memory\", \"10g\") .\\\n",
    "    config(\"spark.executor.memory\", \"64g\") .\\\n",
    "    config(\"spark.driver.maxResultSize\", \"5g\") .\\\n",
    "    config(\"spark.network.timeout\", \"1000s\") .\\\n",
    "    config(\"spark.kryoserializer.buffer.max\", \"1024\") .\\\n",
    "    config(\"spark.sql.broadcastTimeout\", \"36000\") .\\\n",
    "    config(\"spark.sql.crossJoin.enabled\", \"true\") .\\\n",
    "    getOrCreate()\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "005d3940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              origin|                Geom|             aerosol|                blue|               green|                 red|                red1|                red2|                red3|                 nir|                red4|               vapor|              cirius|              swir_1|              swir_2|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|file:///hdd2/shan...|POLYGON ((-0.3627...|[247.0, 247.0, 24...|[240.0, 240.0, 23...|[231.0, 231.0, 22...|[234.0, 234.0, 22...|[234.0, 234.0, 23...|[245.0, 245.0, 24...|[250.0, 250.0, 24...|[225.0, 225.0, 21...|[255.0, 255.0, 23...|[229.0, 229.0, 20...|[239.0, 239.0, 23...|[227.0, 227.0, 22...|[255.0, 255.0, 25...|\n",
      "|file:///hdd2/shan...|POLYGON ((12.2137...|[254.0, 254.0, 25...|[248.0, 248.0, 24...|[248.0, 248.0, 24...|[242.0, 242.0, 24...|[247.0, 247.0, 24...|[239.0, 239.0, 23...|[244.0, 244.0, 24...|[232.0, 232.0, 22...|[255.0, 255.0, 25...|[204.0, 204.0, 20...|[177.0, 177.0, 18...|[166.0, 166.0, 19...|[242.0, 242.0, 24...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GEOTIFF_DIR_BS = \"../data2/euro_sat/water/\"\n",
    "train_bs = spark.read.format(\"geotiff\").option(\"dropInvalid\",True).load(GEOTIFF_DIR_BS)\n",
    "water_df = train_bs.selectExpr(\"image.origin as origin\",\"ST_GeomFromWkt(image.wkt) as Geom\", \"image.data as data\", \"image.nBands as bands\")\n",
    "water_df = water_df.selectExpr(\"origin\", \"Geom\", \"RS_Normalize(RS_GetBand(data,1, bands)) as aerosol\", \"RS_Normalize(Rs_GetBand(data, 2, bands)) as blue\", \"RS_Normalize(RS_GetBand(data, 3, bands)) as green\", \"RS_Normalize(Rs_GetBand(data, 4, bands)) as red\", \"RS_Normalize(RS_GetBand(data, 5, bands)) as red1\", \"RS_Normalize(RS_GetBand(data, 6, bands)) as red2\", \"RS_Normalize(RS_GetBand(data, 7, bands)) as red3\", \"RS_Normalize(RS_GetBand(data, 8, bands)) as nir\", \"RS_Normalize(RS_GetBand(data, 9, bands)) as red4\", \"RS_Normalize(RS_GetBand(data, 10, bands)) as vapor\", \"RS_Normalize(RS_GetBand(data, 11, bands)) as cirius\", \"RS_Normalize(RS_GetBand(data, 12, bands)) as swir_1\", \"RS_Normalize(RS_GetBand(data, 13, bands)) as swir_2\")\n",
    "water_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86deaf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|              origin|                Geom|             aerosol|                blue|               green|                 red|                red1|                red2|                red3|                 nir|                red4|               vapor|              cirius|              swir_1|              swir_2|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|file:///hdd2/shan...|POLYGON ((18.0136...|[202.0, 202.0, 20...|[75.0, 75.0, 75.0...|[87.0, 87.0, 87.0...|[48.0, 48.0, 48.0...|[182.0, 182.0, 18...|[225.0, 225.0, 22...|[209.0, 209.0, 20...|[210.0, 210.0, 22...|[222.0, 222.0, 21...|[212.0, 212.0, 21...|[155.0, 155.0, 15...|[81.0, 81.0, 82.0...|[210.0, 210.0, 20...|\n",
      "|file:///hdd2/shan...|POLYGON ((-3.6443...|[202.0, 202.0, 20...|[142.0, 142.0, 13...|[127.0, 127.0, 12...|[106.0, 106.0, 10...|[147.0, 147.0, 14...|[159.0, 159.0, 15...|[145.0, 145.0, 14...|[138.0, 138.0, 13...|[213.0, 213.0, 21...|[198.0, 198.0, 19...|[158.0, 158.0, 15...|[141.0, 141.0, 13...|[150.0, 150.0, 14...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GEOTIFF_DIR_BS = \"../data2/euro_sat/land/\"\n",
    "train_bs = spark.read.format(\"geotiff\").option(\"dropInvalid\",True).load(GEOTIFF_DIR_BS)\n",
    "land_df = train_bs.selectExpr(\"image.origin as origin\",\"ST_GeomFromWkt(image.wkt) as Geom\", \"image.data as data\", \"image.nBands as bands\")\n",
    "land_df = land_df.selectExpr(\"origin\", \"Geom\", \"RS_Normalize(RS_GetBand(data,1, bands)) as aerosol\", \"RS_Normalize(Rs_GetBand(data, 2, bands)) as blue\", \"RS_Normalize(RS_GetBand(data, 3, bands)) as green\", \"RS_Normalize(Rs_GetBand(data, 4, bands)) as red\", \"RS_Normalize(RS_GetBand(data, 5, bands)) as red1\", \"RS_Normalize(RS_GetBand(data, 6, bands)) as red2\", \"RS_Normalize(RS_GetBand(data, 7, bands)) as red3\", \"RS_Normalize(RS_GetBand(data, 8, bands)) as nir\", \"RS_Normalize(RS_GetBand(data, 9, bands)) as red4\", \"RS_Normalize(RS_GetBand(data, 10, bands)) as vapor\", \"RS_Normalize(RS_GetBand(data, 11, bands)) as cirius\", \"RS_Normalize(RS_GetBand(data, 12, bands)) as swir_1\", \"RS_Normalize(RS_GetBand(data, 13, bands)) as swir_2\")\n",
    "land_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f9cda54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:===================================================> (597 + 13) / 610]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.26897144317627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "GEOTIFF_DIR_BS = \"../data2/euro_sat/load_data/\"\n",
    "train_bs = spark.read.format(\"geotiff\").option(\"dropInvalid\",True).load(GEOTIFF_DIR_BS)\n",
    "train_df = train_bs.selectExpr(\"image.origin as origin\",\"ST_GeomFromWkt(image.wkt) as Geom\", \"image.data as data\", \"image.nBands as bands\")\n",
    "train_df = train_df.selectExpr(\"origin\", \"Geom\", \"RS_Normalize(RS_GetBand(data,1, bands)) as aerosol\", \"RS_Normalize(Rs_GetBand(data, 2, bands)) as blue\", \"RS_Normalize(RS_GetBand(data, 3, bands)) as green\", \"RS_Normalize(Rs_GetBand(data, 4, bands)) as red\", \"RS_Normalize(RS_GetBand(data, 5, bands)) as red1\", \"RS_Normalize(RS_GetBand(data, 6, bands)) as red2\", \"RS_Normalize(RS_GetBand(data, 7, bands)) as red3\", \"RS_Normalize(RS_GetBand(data, 8, bands)) as nir\", \"RS_Normalize(RS_GetBand(data, 9, bands)) as red4\", \"RS_Normalize(RS_GetBand(data, 10, bands)) as vapor\", \"RS_Normalize(RS_GetBand(data, 11, bands)) as cirius\", \"RS_Normalize(RS_GetBand(data, 12, bands)) as swir_1\", \"RS_Normalize(RS_GetBand(data, 13, bands)) as swir_2\")\n",
    "t1 = time.time()\n",
    "train_df.count()\n",
    "t2 = time.time()\n",
    "\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1402377",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19500"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "water_df = water_df.withColumn('class', lit(0))\n",
    "land_df = land_df.withColumn('class', lit(1))\n",
    "\n",
    "train_df = water_df.union(land_df)\n",
    "train_df.persist().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7514ba89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/27 17:47:07 WARN SimpleFunctionRegistry: The function st_ndwi replaced a previously registered function.\n",
      "22/04/27 17:47:07 WARN SimpleFunctionRegistry: The function st_mndwi replaced a previously registered function.\n",
      "22/04/27 17:47:07 WARN SimpleFunctionRegistry: The function st_ndmi replaced a previously registered function.\n",
      "22/04/27 17:47:07 WARN SimpleFunctionRegistry: The function st_ndvi replaced a previously registered function.\n",
      "22/04/27 17:47:07 WARN SimpleFunctionRegistry: The function st_awei replaced a previously registered function.\n",
      "22/04/27 17:47:07 WARN SimpleFunctionRegistry: The function st_bi replaced a previously registered function.\n",
      "22/04/27 17:47:07 WARN SimpleFunctionRegistry: The function st_rvi replaced a previously registered function.\n",
      "22/04/27 17:47:07 WARN SimpleFunctionRegistry: The function st_average replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19500"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spectral indices\n",
    "import random\n",
    "\n",
    "def ndwi(green, nir):\n",
    "    result = []\n",
    "    for i, r in enumerate(green):\n",
    "        if r + nir[i] == 0:\n",
    "            wi = random.uniform(-1, 1)\n",
    "        else:\n",
    "            wi = (r - nir[i])/(r + nir[i])\n",
    "        result.append(wi)\n",
    "    return result\n",
    "\n",
    "def mndwi(green, swir):\n",
    "    result = []\n",
    "    for i, r in enumerate(green):\n",
    "        wi = (r - swir[i])/(r + swir[i])\n",
    "        result.append(wi)\n",
    "    return result\n",
    "\n",
    "\n",
    "def ndmi(nir, swir):\n",
    "    result = []\n",
    "    for i, r in enumerate(nir):\n",
    "        wi = (r - swir[i])/(r + swir[i])\n",
    "        result.append(wi)\n",
    "    return result\n",
    "\n",
    "def ndvi(nir, red):\n",
    "    result = []\n",
    "    for i, r in enumerate(nir):\n",
    "        if r + red[i] == 0:\n",
    "            wi = random.uniform(-1,1)\n",
    "        else:\n",
    "            wi = (r - red[i])/(r + red[i])\n",
    "        result.append(wi)\n",
    "    return result\n",
    "# 4 * (Green - SWIR2) - (0.25 * NIR + 2.75 * SWIR1)\n",
    "\n",
    "\n",
    "def awei(green, swir2, nir, swir1):\n",
    "    result = []\n",
    "    for i, r in enumerate(green):\n",
    "        result.append(4*(r - swir2[i]) - (0.25*nir[i] + 2.75*swir1[i]))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def builtup_index(swir, nir):\n",
    "    result = []\n",
    "    for i, r in enumerate(swir):\n",
    "        if r + nir[i] == 0:\n",
    "            result.append(random.uniform(-1, 1))\n",
    "        else:\n",
    "            result.append((r - nir[i])/(r + nir[i]))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def rvi(nir, red):\n",
    "    result = []\n",
    "    for i, r in enumerate(nir):\n",
    "        if red[i] == 0:\n",
    "            result.append(random.uniform(-1, 1))\n",
    "        else:\n",
    "            result.append(r/red[i])\n",
    "    return result\n",
    "\n",
    "def average(data):\n",
    "    return sum(data)/len(data)\n",
    "   \n",
    "\n",
    "spark.udf.register(\"ST_NDWI\", ndwi, ArrayType(DoubleType()))\n",
    "spark.udf.register(\"ST_MNDWI\", mndwi, ArrayType(DoubleType()))\n",
    "spark.udf.register(\"ST_NDMI\", ndmi, ArrayType(DoubleType()))\n",
    "spark.udf.register(\"ST_NDVI\", ndvi, ArrayType(DoubleType()))\n",
    "spark.udf.register(\"ST_AWEI\", awei, ArrayType(DoubleType()))\n",
    "spark.udf.register(\"ST_BI\", builtup_index, ArrayType(DoubleType()))\n",
    "spark.udf.register(\"ST_RVI\", rvi, ArrayType(DoubleType()))\n",
    "spark.udf.register(\"ST_Average\", average, DoubleType())\n",
    "\n",
    "df_index = train_df.selectExpr(\"origin\", \"ST_Average(ST_NDWI(green, nir)) as ndwi\", \"ST_Average(ST_MNDWI(green, swir_1)) as mdwi\", \"ST_Average(ST_NDMI(nir, swir_1)) as ndmi\", \"ST_Average(ST_NDVI(nir, red)) as ndvi\", \"ST_Average(ST_AWEI(green, swir_2, nir, swir_1)) as awei\", \"ST_Average(ST_BI(swir_1, nir)) as bi\", \"ST_Average(ST_RVI(nir, red)) as rvi\")\n",
    "df_index.count()        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce305017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/27 17:47:22 WARN SimpleFunctionRegistry: The function rs_convert replaced a previously registered function.\n",
      "22/04/27 17:47:22 WARN SimpleFunctionRegistry: The function glcm_contrast replaced a previously registered function.\n",
      "22/04/27 17:47:22 WARN SimpleFunctionRegistry: The function glcm_dis replaced a previously registered function.\n",
      "22/04/27 17:47:22 WARN SimpleFunctionRegistry: The function glcm_hom replaced a previously registered function.\n",
      "22/04/27 17:47:22 WARN SimpleFunctionRegistry: The function glcm_asm replaced a previously registered function.\n",
      "22/04/27 17:47:22 WARN SimpleFunctionRegistry: The function glcm_corr replaced a previously registered function.\n",
      "22/04/27 17:47:22 WARN SimpleFunctionRegistry: The function glcm_energy replaced a previously registered function.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19500"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' textural features from last dataset  '''\n",
    "\n",
    "def rgbtograyscale(red, green, blue):\n",
    "    a = (0.299*np.array(red) + 0.587*np.array(green) + 0.114*np.array(blue)).tolist()\n",
    "    b = [round(x) for x in a]\n",
    "    return b\n",
    "\n",
    "def GLCM_Contrast(pixels):\n",
    "    mi, ma = 0, 255\n",
    "    ks = 5\n",
    "    h,w = 64,64\n",
    "    nbit = 8\n",
    "    bins = np.linspace(mi, ma+1, nbit+1)\n",
    "    bin_image = np.digitize(pixels, bins) - 1\n",
    "    new_image = np.reshape(bin_image, (64, 64))\n",
    "    glcm = greycomatrix(new_image, [1], [0, np.pi/4, np.pi/2], levels=8 , normed=True, symmetric=True)\n",
    "    feature = gc(glcm, \"contrast\")\n",
    "    return sum(feature[0].tolist())/len(feature[0].tolist())\n",
    "\n",
    "def GLCM_Dissimilarity(pixels):\n",
    "    mi, ma = 0, 255\n",
    "    ks = 5\n",
    "    h,w = 64,64\n",
    "    nbit = 8\n",
    "    bins = np.linspace(mi, ma+1, nbit+1)\n",
    "    bin_image = np.digitize(pixels, bins) - 1\n",
    "    new_image = np.reshape(bin_image, (64,64))\n",
    "    glcm = greycomatrix(new_image, [1], [0, np.pi/4, np.pi/2], levels=8 , normed=True, symmetric=True)\n",
    "    feature = gc(glcm, \"dissimilarity\")\n",
    "    return sum(feature[0].tolist())/len(feature[0].tolist())\n",
    "\n",
    "\n",
    "def GLCM_Homogeneity(pixels):\n",
    "    mi, ma = 0, 255\n",
    "    ks = 5\n",
    "    h,w = 64,64\n",
    "    nbit = 8\n",
    "    bins = np.linspace(mi, ma+1, nbit+1)\n",
    "    bin_image = np.digitize(pixels, bins) - 1\n",
    "    new_image = np.reshape(bin_image, (64,64))\n",
    "    glcm = greycomatrix(new_image, [1], [0, np.pi/4, np.pi/2], levels=8 , normed=True, symmetric=True)\n",
    "    feature = gc(glcm, \"homogeneity\")\n",
    "    return sum(feature[0].tolist())/len(feature[0].tolist())\n",
    "\n",
    "def GLCM_Energy(pixels):\n",
    "    mi, ma = 0, 255\n",
    "    ks = 5\n",
    "    h,w = 64,64\n",
    "    nbit = 8\n",
    "    bins = np.linspace(mi, ma+1, nbit+1)\n",
    "    bin_image = np.digitize(pixels, bins) - 1\n",
    "    new_image = np.reshape(bin_image, (64,64))\n",
    "    glcm = greycomatrix(new_image, [1], [0, np.pi/4, np.pi/2], levels=8 , normed=True, symmetric=True)\n",
    "    feature = gc(glcm, \"energy\")\n",
    "    return sum(feature[0].tolist())/len(feature[0].tolist())\n",
    "\n",
    "\n",
    "def GLCM_Correlation(pixels):\n",
    "    mi, ma = 0, 255\n",
    "    ks = 5\n",
    "    h,w = 64,64\n",
    "    nbit = 8\n",
    "    bins = np.linspace(mi, ma+1, nbit+1)\n",
    "    bin_image = np.digitize(pixels, bins) - 1\n",
    "    new_image = np.reshape(bin_image, (64,64))\n",
    "    glcm = greycomatrix(new_image, [1], [0, np.pi/4, np.pi/2], levels=8 , normed=True, symmetric=True)\n",
    "    feature = gc(glcm, \"correlation\")\n",
    "    return sum(feature[0].tolist())/len(feature[0].tolist())\n",
    "\n",
    "def GLCM_ASM(pixels):\n",
    "    mi, ma = 0, 255\n",
    "    ks = 5\n",
    "    h,w = 64,64\n",
    "    nbit = 8\n",
    "    bins = np.linspace(mi, ma+1, nbit+1)\n",
    "    bin_image = np.digitize(pixels, bins) - 1\n",
    "    new_image = np.reshape(bin_image, (64,64))\n",
    "    glcm = greycomatrix(new_image, [1], [np.pi/4,np.pi/2], levels=8 , normed=True, symmetric=True)\n",
    "    feature = gc(glcm, \"ASM\")\n",
    "    return sum(feature[0].tolist())/len(feature[0].tolist())\n",
    "\n",
    "\n",
    "spark.udf.register(\"RS_Convert\", rgbtograyscale, ArrayType(IntegerType()))\n",
    "spark.udf.register(\"GLCM_Contrast\", GLCM_Contrast,DoubleType())\n",
    "spark.udf.register(\"GLCM_Dis\", GLCM_Dissimilarity, DoubleType())\n",
    "spark.udf.register(\"GLCM_Hom\", GLCM_Homogeneity, DoubleType())\n",
    "spark.udf.register(\"GLCM_ASM\", GLCM_ASM, DoubleType())\n",
    "spark.udf.register(\"GLCM_Corr\", GLCM_Correlation, DoubleType())\n",
    "spark.udf.register(\"GLCM_Energy\", GLCM_Energy, DoubleType())\n",
    "\n",
    "gray_scale_dF = train_df.selectExpr(\"origin\", \"RS_Convert(red, green, blue) as gray_scale\", \"class\")\n",
    "text_df = gray_scale_dF.selectExpr(\"origin\", \"GLCM_Contrast(gray_scale) as glcm_contrast\",\"GLCM_Dis(gray_scale) as glcm_dissimilarity\", \"GLCM_Hom(gray_scale) as glcm_homogeneity\", \"GLCM_Energy(gray_scale) as glcm_energy\", \"GLCM_Corr(gray_scale) as glcm_correlation\",\"GLCM_ASM(gray_scale) as glcm_ASM\", \"class\")\n",
    "text_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b6519f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indices_df = df_index.toPandas()\n",
    "textural_df = text_df.toPandas()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "907d9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_textural_df = indices_df.merge(textural_df, on=\"origin\", how=\"inner\")\n",
    "final_textural_df.to_csv(\"../data2/euro_sat/textural_features/textural.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9578fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            geomText|                tags|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|13.56900320000000...|[{communication:m...|\n",
      "|  2|30.32611000000000...|[{historic, memor...|\n",
      "| 69|12.86222370000000...|   [{natural, tree}]|\n",
      "|100|10.8340913,52.891...|[{description, We...|\n",
      "|110|10.78341500000000...|                  []|\n",
      "|111|10.7875238,59.947...|                  []|\n",
      "|112|10.7854259,59.951...|[{barrier, block}...|\n",
      "|113|10.78193170000000...|                  []|\n",
      "|114|10.78433900000000...|                  []|\n",
      "|115|10.7796921,59.951...|                  []|\n",
      "|116|10.7781722,59.952...|                  []|\n",
      "|117|10.78134160000000...|                  []|\n",
      "|118|10.7856298,59.951...|                  []|\n",
      "|119|10.7862779,59.951...|                  []|\n",
      "|120|10.78726150000000...|                  []|\n",
      "|121|10.7892711,59.950...|                  []|\n",
      "|122|10.7898343,59.950...|                  []|\n",
      "|123|10.7899198,59.950...|                  []|\n",
      "|125|10.7901962,59.949...|                  []|\n",
      "|127|10.7897583,59.947...|                  []|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "| waysId|          array_geom|            Geometry|\n",
      "+-------+--------------------+--------------------+\n",
      "|    474|[[1, -0.805012800...|-0.80630750000000...|\n",
      "|    964|[[0, -1.798361900...|-1.79836190000000...|\n",
      "|   1677|[[1, -1.3948659,5...|-1.3941614,50.926...|\n",
      "|   2214|[[0, -0.0166724,5...|-0.0166724,51.426...|\n",
      "|   2509|[[3, -1.834021000...|-1.8339437,52.650...|\n",
      "|   2529|[[5, -2.3599828,5...|-2.35976750000000...|\n",
      "|   2927|[[10, -0.03121260...|-0.02936080000000...|\n",
      "|  83646|[[17, -4.79418370...|-4.78877960000000...|\n",
      "| 252993|[[0, -1.372270900...|-1.37227090000000...|\n",
      "| 260048|[[1, -0.492700200...|-0.4929179,51.370...|\n",
      "| 276858|[[4, -0.1928493,5...|-0.1948544,51.602...|\n",
      "| 314372|[[3, 8.4701543,55...|8.4702433,55.4762...|\n",
      "| 363447|[[2, 18.0744036,5...|18.0742134,59.340...|\n",
      "| 374036|[[1, 16.2739489,4...|16.27396790000000...|\n",
      "| 599927|[[6, -1.5417018,5...|-1.5413977,50.676...|\n",
      "| 838032|[[4, -2.841060700...|-2.8411778,53.185...|\n",
      "| 959021|[[5, -0.4298426,5...|-0.428368,52.1546...|\n",
      "|1210248|[[7, -1.1586706,5...|-1.1585307,50.652...|\n",
      "|1212316|[[0, 8.4603778,55...|8.4603778,55.4686...|\n",
      "|1215397|[[4, -1.1582884,5...|-1.1594449,50.657...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+---------------+\n",
      "| id|            Geometry|   attr_key|     attr_value|\n",
      "+---+--------------------+-----------+---------------+\n",
      "|474|POLYGON ((-0.8063...|designation|public_footpath|\n",
      "|474|POLYGON ((-0.8063...|       foot|            yes|\n",
      "+---+--------------------+-----------+---------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/hdd2/shantanuCodeData/data/pbf/euro_sat/\"\n",
    "nodes = spark.read.parquet(DATA_DIR + \"europe-latest.osm.pbf.node.parquet\")\n",
    "nodes = nodes.selectExpr(\"id\",\"tags\",\"CONCAT(longitude, ',',  latitude) as geomText\")\n",
    "nodes = nodes.select(\"id\",\"geomText\",\"tags\")\n",
    "nodes.persist().show()\n",
    "\n",
    "def getlen(col):\n",
    "    return len(col)\n",
    "\n",
    "def regUDF(spark):\n",
    "    length = udf(getlen, IntegerType())\n",
    "    spark.udf.register(\"ColLen\", length)\n",
    "\n",
    "def constructGeometry(col):\n",
    "    temp = []\n",
    "    col = sorted(col, key=lambda x: int(x[0]))\n",
    "    first_geom = col[0][1]\n",
    "    for arr in col:\n",
    "        temp.append(arr[1])\n",
    "    temp.append(first_geom)\n",
    "    return \",\".join(temp)\n",
    "    \n",
    "\n",
    "def reggeomUDF(spark):\n",
    "    geom = udf(constructGeometry, StringType())\n",
    "    spark.udf.register(\"GetGeom\", geom)\n",
    "\n",
    "ways = spark.read.parquet(DATA_DIR + \"europe-latest.osm.pbf.way.parquet\")\n",
    "ways = ways.select(\"id\",\"tags\",explode(\"nodes\")).select(\n",
    "  \"*\", col(\"col\")[\"index\"].alias(\"index\"), col(\"col\")[\"nodeId\"].alias(\"nodeId\")\n",
    "    )\n",
    "ways.createOrReplaceTempView(\"ways\")\n",
    "nodes.createOrReplaceTempView(\"points\")\n",
    "waysJoinnodes = spark.sql(\"Select ways.id as waysId,points.id as nodeId,ways.index as nodeindex, ways.tags as tags,points.geomText as geomText from points JOIN ways ON points.id=ways.nodeId\")\n",
    "waysJoinnodes = waysJoinnodes.withColumn(\"index_geom\", f.array(f.col(\"nodeindex\"), f.col(\"geomText\"))).select(\"waysId\", \"nodeId\", \"tags\", \"index_geom\")\n",
    "waysJoinnodes = waysJoinnodes.groupBy(\"waysId\").agg(collect_set(\"index_geom\").alias('array_geom'))\n",
    "regUDF(spark)\n",
    "reggeomUDF(spark)\n",
    "waysJoinnodes = waysJoinnodes.selectExpr(\"waysId\", \"array_geom\",\"ColLen(array_geom) as length\").where(\"length>=3\").selectExpr(\"waysId\", \"array_geom\", \"GetGeom(array_geom) as Geometry\")\n",
    "waysJoinnodes.persist().show()\n",
    "waysJoinnodes.createOrReplaceTempView(\"waysjoin\")\n",
    "finalways = spark.sql(\"Select ways.id as id, ways.tags as tags, waysjoin.Geometry as Geometry from ways JOIN waysjoin ON ways.id=waysjoin.waysId \")\n",
    "finalways =  finalways.distinct().select(\"id\", explode(\"tags\"), \"Geometry\")\n",
    "nodes = nodes.select(\"id\",\"geomText\",explode(\"tags\")).selectExpr(\"id\",\"ST_PointFromText(geomtext, ',') as Geometry\", \"col as attribute\")\n",
    "nodes.createOrReplaceTempView(\"nodes\")\n",
    "ways = finalways.selectExpr(\"id\",\"ST_PolygonFromText(Geometry, ',') as Geometry\", \"col as attribute\")\n",
    "desiredCoordinates = nodes.select(\n",
    "  \"*\", col(\"attribute\")[\"key\"].alias(\"key\"), col(\"attribute\")[\"value\"].alias(\"value\"))\n",
    "ways = ways.select(\n",
    "  \"*\", col(\"attribute\")[\"key\"].alias(\"key\"), col(\"attribute\")[\"value\"].alias(\"value\"))\n",
    "def getString(hexa):\n",
    "    return hexa.decode()\n",
    "convert = udf(getString, StringType())\n",
    "spark.udf.register(\"RS_Convert\", convert)\n",
    "points = desiredCoordinates.selectExpr(\"id\", \"Geometry\",\"RS_Convert(key) as attr_key\",\"RS_Convert(value) as attr_value\")\n",
    "polygons = ways.selectExpr(\"id\", \"Geometry\",\"RS_Convert(key) as attr_key\",\"RS_Convert(value) as attr_value\")\n",
    "# desiredCoordinates.unpersist()\n",
    "# points.persist().show(2)\n",
    "polygons.persist().show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81006ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# points.write.parquet(\"/hdd2/shantanuCodeData/data2/euro_sat/kb/nodes.parquet\")\n",
    "polygons.write.parquet(\"/hdd2/shantanuCodeData/data2/euro_sat/kb/ways.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a2501",
   "metadata": {},
   "source": [
    "# Geospatial features using OSM knowledge vase of europe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b68cae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+\n",
      "|          origin|                Geom|class|\n",
      "+----------------+--------------------+-----+\n",
      "|SeaLake_2847.tif|POLYGON ((-0.3627...|    0|\n",
      "| SeaLake_363.tif|POLYGON ((12.2137...|    0|\n",
      "|  River_2019.tif|POLYGON ((9.27551...|    0|\n",
      "|SeaLake_2111.tif|POLYGON ((8.32115...|    0|\n",
      "| SeaLake_734.tif|POLYGON ((5.21409...|    0|\n",
      "| SeaLake_430.tif|POLYGON ((28.0818...|    0|\n",
      "|SeaLake_1243.tif|POLYGON ((14.1621...|    0|\n",
      "| SeaLake_709.tif|POLYGON ((4.33435...|    0|\n",
      "|SeaLake_1891.tif|POLYGON ((0.64670...|    0|\n",
      "|   River_495.tif|POLYGON ((0.80404...|    0|\n",
      "|SeaLake_2832.tif|POLYGON ((-1.6822...|    0|\n",
      "|SeaLake_2583.tif|POLYGON ((20.0949...|    0|\n",
      "|SeaLake_2643.tif|POLYGON ((4.11489...|    0|\n",
      "|  River_2095.tif|POLYGON ((9.58068...|    0|\n",
      "|SeaLake_2582.tif|POLYGON ((25.4956...|    0|\n",
      "|SeaLake_2657.tif|POLYGON ((10.4027...|    0|\n",
      "|SeaLake_2834.tif|POLYGON ((20.9071...|    0|\n",
      "|SeaLake_2317.tif|POLYGON ((14.3050...|    0|\n",
      "|  River_2032.tif|POLYGON ((4.87967...|    0|\n",
      "|SeaLake_1930.tif|POLYGON ((9.36970...|    0|\n",
      "+----------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_origin(text):\n",
    "    return text.split(\"/\")[-1]\n",
    "spark.udf.register(\"ST_Origin\", get_origin, StringType())\n",
    "\n",
    "\n",
    "geo_train_df = train_df.selectExpr(\"ST_Origin(origin) as origin\", \"Geom\", \"class\")\n",
    "geo_train_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d4bf2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:================================================>       (13 + 2) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+----------+\n",
      "|        id|            Geometry|attr_key|attr_value|\n",
      "+----------+--------------------+--------+----------+\n",
      "|3123477544|POINT (12.2705131...| landuse|    forest|\n",
      "| 395384461|POINT (2.4340426 ...| landuse|    forest|\n",
      "|4757465299|POINT (9.0373967 ...| landuse|    forest|\n",
      "|4767431496|POINT (9.495564 4...| landuse|    forest|\n",
      "|4767480875|POINT (9.4964887 ...| landuse|    forest|\n",
      "|4768270166|POINT (9.4990301 ...| landuse|    forest|\n",
      "|4768270167|POINT (9.4992085 ...| landuse|    forest|\n",
      "| 295876412|POINT (11.0076779...| landuse|    forest|\n",
      "| 295876414|POINT (10.9949740...| landuse|    forest|\n",
      "| 295876415|POINT (11.0759655...| landuse|    forest|\n",
      "| 295876416|POINT (11.0781206...| landuse|    forest|\n",
      "| 295876949|POINT (11.0218181...| landuse|    forest|\n",
      "| 470624533|POINT (9.2603281 ...| landuse|    forest|\n",
      "| 470949988|POINT (13.8594593...| landuse|    forest|\n",
      "| 470950419|POINT (13.9156899...| landuse|    forest|\n",
      "| 471127707|POINT (10.2174569...| landuse|    forest|\n",
      "| 489558084|POINT (13.0564880...| landuse|    forest|\n",
      "| 669420749|POINT (-0.0769674...| landuse|    forest|\n",
      "| 675090456|POINT (9.2351243 ...| landuse|    forest|\n",
      "| 675106423|POINT (9.2833951 ...| landuse|    forest|\n",
      "+----------+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:====================================================>   (14 + 1) / 15]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "points = spark.read.parquet(\"../data2/euro_sat/kb/nodes.parquet\")\n",
    "polygons = spark.read.parquet(\"../data2/euro_sat/kb/ways.parquet\")\n",
    "\n",
    "joint_kb = points.union(polygons)\n",
    "\n",
    "forest_points = points.filter((points.attr_key==\"landuse\") & (points.attr_value==\"forest\"))\n",
    "forest_polygons = polygons.filter((polygons.attr_key==\"landuse\") & (polygons.attr_value==\"forest\"))\n",
    "\n",
    "forest_kb = forest_points.union(forest_polygons)\n",
    "forest_kb.persist().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "344eea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+\n",
      "|          origin|                Geom|class|\n",
      "+----------------+--------------------+-----+\n",
      "|SeaLake_2847.tif|POLYGON ((-0.3627...|    0|\n",
      "| SeaLake_363.tif|POLYGON ((12.2137...|    0|\n",
      "+----------------+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------------+-----------------+-----+\n",
      "|          origin|isOverlaps_forest|class|\n",
      "+----------------+-----------------+-----+\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "+----------------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/23 21:36:24 WARN CacheManager: Asked to cache already cached data.\n",
      "22/04/23 21:36:24 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+\n",
      "|          origin|   distance_forest|class|\n",
      "+----------------+------------------+-----+\n",
      "|SeaLake_2847.tif| 15.92138426593651|    0|\n",
      "| SeaLake_363.tif|6.9401393240031615|    0|\n",
      "|  River_2019.tif|   6.0205898028452|    0|\n",
      "|SeaLake_2111.tif| 5.231899424653846|    0|\n",
      "| SeaLake_734.tif| 7.766154578466899|    0|\n",
      "| SeaLake_430.tif|16.211476666124316|    0|\n",
      "|SeaLake_1243.tif| 9.130463626835864|    0|\n",
      "| SeaLake_709.tif| 7.963704188046001|    0|\n",
      "|SeaLake_1891.tif|11.872130347015137|    0|\n",
      "|   River_495.tif|11.463836166350218|    0|\n",
      "|SeaLake_2832.tif|14.081808685514021|    0|\n",
      "|SeaLake_2583.tif| 8.287736427012202|    0|\n",
      "|SeaLake_2643.tif| 8.596076084428088|    0|\n",
      "|  River_2095.tif|2.8234581237121774|    0|\n",
      "|SeaLake_2582.tif|18.009620181670627|    0|\n",
      "|SeaLake_2657.tif|5.5746050154751305|    0|\n",
      "|SeaLake_2834.tif| 12.76047246395043|    0|\n",
      "|SeaLake_2317.tif| 9.167645154537414|    0|\n",
      "|  River_2032.tif|  7.71836638868871|    0|\n",
      "|SeaLake_1930.tif| 4.488537345954093|    0|\n",
      "+----------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/23 21:36:24 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+\n",
      "|          origin|isIntersect_forest|class|\n",
      "+----------------+------------------+-----+\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "| SeaLake_363.tif|                 0|    0|\n",
      "|  River_2019.tif|                 0|    0|\n",
      "|SeaLake_2111.tif|                 0|    0|\n",
      "| SeaLake_734.tif|                 0|    0|\n",
      "| SeaLake_430.tif|                 0|    0|\n",
      "|SeaLake_1243.tif|                 0|    0|\n",
      "| SeaLake_709.tif|                 0|    0|\n",
      "|SeaLake_1891.tif|                 0|    0|\n",
      "|   River_495.tif|                 0|    0|\n",
      "|SeaLake_2832.tif|                 0|    0|\n",
      "|SeaLake_2583.tif|                 0|    0|\n",
      "|SeaLake_2643.tif|                 0|    0|\n",
      "|  River_2095.tif|                 0|    0|\n",
      "|SeaLake_2582.tif|                 0|    0|\n",
      "|SeaLake_2657.tif|                 0|    0|\n",
      "|SeaLake_2834.tif|                 0|    0|\n",
      "|SeaLake_2317.tif|                 0|    0|\n",
      "|  River_2032.tif|                 0|    0|\n",
      "|SeaLake_1930.tif|                 0|    0|\n",
      "+----------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/23 21:36:24 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----+\n",
      "|          origin|isContain_forest|class|\n",
      "+----------------+----------------+-----+\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "| SeaLake_363.tif|               0|    0|\n",
      "|  River_2019.tif|               0|    0|\n",
      "|SeaLake_2111.tif|               0|    0|\n",
      "| SeaLake_734.tif|               0|    0|\n",
      "| SeaLake_430.tif|               0|    0|\n",
      "|SeaLake_1243.tif|               0|    0|\n",
      "| SeaLake_709.tif|               0|    0|\n",
      "|SeaLake_1891.tif|               0|    0|\n",
      "|   River_495.tif|               0|    0|\n",
      "|SeaLake_2832.tif|               0|    0|\n",
      "|SeaLake_2583.tif|               0|    0|\n",
      "|SeaLake_2643.tif|               0|    0|\n",
      "|  River_2095.tif|               0|    0|\n",
      "|SeaLake_2582.tif|               0|    0|\n",
      "|SeaLake_2657.tif|               0|    0|\n",
      "|SeaLake_2834.tif|               0|    0|\n",
      "|SeaLake_2317.tif|               0|    0|\n",
      "|  River_2032.tif|               0|    0|\n",
      "|SeaLake_1930.tif|               0|    0|\n",
      "+----------------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/23 21:36:25 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----+\n",
      "|          origin|isTouches_forest|class|\n",
      "+----------------+----------------+-----+\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "| SeaLake_363.tif|               0|    0|\n",
      "|  River_2019.tif|               0|    0|\n",
      "|SeaLake_2111.tif|               0|    0|\n",
      "| SeaLake_734.tif|               0|    0|\n",
      "| SeaLake_430.tif|               0|    0|\n",
      "|SeaLake_1243.tif|               0|    0|\n",
      "| SeaLake_709.tif|               0|    0|\n",
      "|SeaLake_1891.tif|               0|    0|\n",
      "|   River_495.tif|               0|    0|\n",
      "|SeaLake_2832.tif|               0|    0|\n",
      "|SeaLake_2583.tif|               0|    0|\n",
      "|SeaLake_2643.tif|               0|    0|\n",
      "|  River_2095.tif|               0|    0|\n",
      "|SeaLake_2582.tif|               0|    0|\n",
      "|SeaLake_2657.tif|               0|    0|\n",
      "|SeaLake_2834.tif|               0|    0|\n",
      "|SeaLake_2317.tif|               0|    0|\n",
      "|  River_2032.tif|               0|    0|\n",
      "|SeaLake_1930.tif|               0|    0|\n",
      "+----------------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Spatial joins with sealakes, river, forest, vegeration, highway, industrial and residential data points '''\n",
    "geo_train_df.show(2)\n",
    "geo_train_df.createOrReplaceTempView(\"train\")\n",
    "\n",
    "forest_kb.createOrReplaceTempView(\"forest_kb\")\n",
    "forest_kb.createOrReplaceTempView(\"forest_kb\")\n",
    "forest_overlap = spark.sql(\"select origin, cast(ST_Overlaps(train.geom, forest_kb.Geometry) as INT) as isOverlaps_forest, class from train, forest_kb\") \n",
    "forest_distance = spark.sql(\"select origin, ST_Distance(train.geom, forest_kb.Geometry) as distance_forest, class from train, forest_kb\")\n",
    "forest_intersects = spark.sql(\"select origin, cast(ST_Intersects(train.geom, forest_kb.Geometry) as INT) as isIntersect_forest, class from train, forest_kb\") \n",
    "forest_contains = spark.sql(\"select origin, cast(ST_Contains(train.geom, forest_kb.Geometry) as INT) as isContain_forest, class from train, forest_kb\") \n",
    "forest_touches = spark.sql(\"select origin, cast(ST_Touches(train.geom, forest_kb.Geometry) as INT) as isTouches_forest, class from train, forest_kb\") \n",
    "\n",
    "forest_overlap.persist().show()\n",
    "forest_distance.persist().show()\n",
    "forest_intersects.persist().show()\n",
    "forest_contains.persist().show()\n",
    "forest_touches.persist().show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb041535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19500"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = forest_overlap.groupby(\"origin\").sum(\"isOverlaps_forest\").alias(\"total_overlap_forest\")\n",
    "df_2 = forest_distance.groupby(\"origin\").min(\"distance_forest\").alias(\"distance_forest\")\n",
    "df_3 = forest_contains.groupby(\"origin\").sum(\"isContain_forest\").alias(\"total_contain_forest\")\n",
    "df_4 = forest_touches.groupby(\"origin\").sum(\"isTouches_forest\").alias(\"total_touch_forest\")\n",
    "df_5 = forest_intersects.groupby(\"origin\").sum(\"isIntersect_forest\").alias(\"total_intersect_forest\")\n",
    "\n",
    "df_1.persist().count()\n",
    "df_2.persist().count()\n",
    "df_3.persist().count()\n",
    "df_4.persist().count()\n",
    "df_5.persist().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9c8a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_1.toPandas()\n",
    "df2 = df_2.toPandas()\n",
    "df3 = df_3.toPandas()\n",
    "df4 = df_4.toPandas()\n",
    "df5 = df_5.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1bc4bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df1.merge(df2, on='origin', how=\"inner\")\n",
    "df_final = df3.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df4.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df5.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "\n",
    "df_final.to_csv(\"../data2/euro_sat/forest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dfa056f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+----------+\n",
      "|        id|            Geometry|attr_key|attr_value|\n",
      "+----------+--------------------+--------+----------+\n",
      "| 343120423|POINT (-1.1806796...| landuse|industrial|\n",
      "| 966347483|POINT (20.8315943...| landuse|industrial|\n",
      "|3122745778|POINT (11.1305472...| landuse|industrial|\n",
      "|3122785289|POINT (11.4080631...| landuse|industrial|\n",
      "|3131028444|POINT (-1.4895529...| landuse|industrial|\n",
      "|3134432145|POINT (16.58014 5...| landuse|industrial|\n",
      "| 385374112|POINT (21.6291288...| landuse|industrial|\n",
      "| 385592207|POINT (-5.8403852...| landuse|industrial|\n",
      "| 386940286|POINT (-2.3005779...| landuse|industrial|\n",
      "| 392105051|POINT (7.1700663 ...| landuse|industrial|\n",
      "|4748573723|POINT (11.7685078...| landuse|industrial|\n",
      "|4748663370|POINT (32.2659704...| landuse|industrial|\n",
      "| 289094210|POINT (0.6890868 ...| landuse|industrial|\n",
      "| 289094211|POINT (0.6872954 ...| landuse|industrial|\n",
      "| 289094212|POINT (0.685662 5...| landuse|industrial|\n",
      "| 289094213|POINT (0.68679090...| landuse|industrial|\n",
      "| 289094225|POINT (0.6886128 ...| landuse|industrial|\n",
      "| 289094226|POINT (0.6890943 ...| landuse|industrial|\n",
      "| 289094232|POINT (0.6937607 ...| landuse|industrial|\n",
      "| 289094233|POINT (0.69334190...| landuse|industrial|\n",
      "+----------+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+-----+\n",
      "|          origin|isOverlaps_industry|class|\n",
      "+----------------+-------------------+-----+\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "+----------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+\n",
      "|          origin| distance_industry|class|\n",
      "+----------------+------------------+-----+\n",
      "|SeaLake_2847.tif| 13.62697702091927|    0|\n",
      "|SeaLake_2847.tif| 21.25205817573253|    0|\n",
      "|SeaLake_2847.tif|23.269014440354436|    0|\n",
      "|SeaLake_2847.tif|23.363002678933412|    0|\n",
      "|SeaLake_2847.tif|14.242571139149645|    0|\n",
      "|SeaLake_2847.tif| 21.50685936722675|    0|\n",
      "|SeaLake_2847.tif|22.997011016253968|    0|\n",
      "|SeaLake_2847.tif|6.7869513399955546|    0|\n",
      "|SeaLake_2847.tif|12.540518801623419|    0|\n",
      "|SeaLake_2847.tif|14.654297186708982|    0|\n",
      "|SeaLake_2847.tif|13.063282779659225|    0|\n",
      "|SeaLake_2847.tif|33.883840560389665|    0|\n",
      "|SeaLake_2847.tif| 12.93463479737444|    0|\n",
      "|SeaLake_2847.tif|12.933956492719917|    0|\n",
      "|SeaLake_2847.tif|12.933876335801349|    0|\n",
      "|SeaLake_2847.tif|12.935325926928197|    0|\n",
      "|SeaLake_2847.tif|12.937544078153012|    0|\n",
      "|SeaLake_2847.tif|12.938038852678567|    0|\n",
      "|SeaLake_2847.tif| 12.93683417403714|    0|\n",
      "|SeaLake_2847.tif|12.936689977564834|    0|\n",
      "+----------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+\n",
      "|          origin|isContain_industry|class|\n",
      "+----------------+------------------+-----+\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "+----------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+\n",
      "|          origin|isIntersect_industry|class|\n",
      "+----------------+--------------------+-----+\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "+----------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----+\n",
      "|          origin|isTouch_industry|class|\n",
      "+----------------+----------------+-----+\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "+----------------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "industrial_points = points.filter((points.attr_key==\"landuse\") & (points.attr_value==\"industrial\"))\n",
    "industrial_polygons = polygons.filter((polygons.attr_key==\"landuse\") & (polygons.attr_value==\"industrial\"))\n",
    "\n",
    "industrial_kb = industrial_points.union(industrial_polygons)\n",
    "industrial_kb.persist().show()\n",
    "industrial_kb.createOrReplaceTempView(\"industry_kb\")\n",
    "industrial_overlap = spark.sql(\"select origin, cast(ST_Overlaps(train.geom, industry_kb.Geometry) as INT) as isOverlaps_industry, class from train, industry_kb\") \n",
    "industrial_distance = spark.sql(\"select origin, ST_Distance(train.geom, industry_kb.Geometry) as distance_industry, class from train, industry_kb\")\n",
    "industrial_contains = spark.sql(\"select origin, cast(ST_Contains(train.geom, industry_kb.Geometry) as INT) as isContain_industry, class from train, industry_kb\") \n",
    "industrial_intersects = spark.sql(\"select origin, cast(ST_Intersects(train.geom, industry_kb.Geometry) as INT) as isIntersect_industry, class from train, industry_kb\") \n",
    "industrial_touches = spark.sql(\"select origin, cast(ST_Touches(train.geom, industry_kb.Geometry) as INT) as isTouch_industry, class from train, industry_kb\") \n",
    "industrial_overlap.persist().show()\n",
    "industrial_distance.persist().show()\n",
    "industrial_contains.persist().show()\n",
    "industrial_intersects.persist().show()\n",
    "industrial_touches.persist().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a8e0ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19500"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_6 = industrial_overlap.groupby(\"origin\").sum(\"isOverlaps_industry\").alias(\"total_overlap_ind\")\n",
    "df_7 = industrial_distance.groupby(\"origin\").min(\"distance_industry\").alias(\"distance_industrial\")\n",
    "df_8 = industrial_contains.groupby(\"origin\").sum(\"isContain_industry\").alias(\"total_contain_industrial\")\n",
    "df_9 = industrial_touches.groupby(\"origin\").sum(\"isTouch_industry\").alias(\"total_touch_industrial\")\n",
    "df_10 = industrial_intersects.groupby(\"origin\").sum(\"isIntersect_industry\").alias(\"total_intersect_industrial\")\n",
    "\n",
    "df_6.persist().count()\n",
    "df_7.persist().count()\n",
    "df_8.persist().count()\n",
    "df_9.persist().count()\n",
    "df_10.persist().count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07bb6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df_6.toPandas()\n",
    "df7 = df_7.toPandas()\n",
    "df8 = df_8.toPandas()\n",
    "df9 = df_9.toPandas()\n",
    "df10 = df_10.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5df1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df6.merge(df7, on='origin', how=\"inner\")\n",
    "df_final = df8.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df9.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df10.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "\n",
    "df_final.to_csv(\"../data2/euro_sat/industry.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88ff78cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/23 20:45:24 WARN CacheManager: Asked to cache already cached data.\n",
      "22/04/23 20:45:24 WARN CacheManager: Asked to cache already cached data.\n",
      "22/04/23 20:45:24 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+-----+\n",
      "|          origin|isOverlaps_residential|class|\n",
      "+----------------+----------------------+-----+\n",
      "|SeaLake_2847.tif|                     0|    0|\n",
      "|SeaLake_2847.tif|                     0|    0|\n",
      "|SeaLake_2847.tif|                     0|    0|\n",
      "| SeaLake_363.tif|                     0|    0|\n",
      "| SeaLake_363.tif|                     0|    0|\n",
      "| SeaLake_363.tif|                     0|    0|\n",
      "|  River_2019.tif|                     0|    0|\n",
      "|  River_2019.tif|                     0|    0|\n",
      "|  River_2019.tif|                     0|    0|\n",
      "|SeaLake_2111.tif|                     0|    0|\n",
      "|SeaLake_2111.tif|                     0|    0|\n",
      "|SeaLake_2111.tif|                     0|    0|\n",
      "| SeaLake_734.tif|                     0|    0|\n",
      "| SeaLake_734.tif|                     0|    0|\n",
      "| SeaLake_734.tif|                     0|    0|\n",
      "| SeaLake_430.tif|                     0|    0|\n",
      "| SeaLake_430.tif|                     0|    0|\n",
      "| SeaLake_430.tif|                     0|    0|\n",
      "|SeaLake_1243.tif|                     0|    0|\n",
      "|SeaLake_1243.tif|                     0|    0|\n",
      "+----------------+----------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+--------------------+-----+\n",
      "|          origin|distance_residential|class|\n",
      "+----------------+--------------------+-----+\n",
      "|SeaLake_2847.tif|  12.050151011427687|    0|\n",
      "|SeaLake_2847.tif|  12.276396740969966|    0|\n",
      "|SeaLake_2847.tif|   12.93214257480359|    0|\n",
      "| SeaLake_363.tif|  14.837671049329336|    0|\n",
      "| SeaLake_363.tif|  15.939413639039824|    0|\n",
      "| SeaLake_363.tif|   16.14039293496527|    0|\n",
      "|  River_2019.tif|   9.139516266985105|    0|\n",
      "|  River_2019.tif|  10.200510013118855|    0|\n",
      "|  River_2019.tif|   9.798061285287638|    0|\n",
      "|SeaLake_2111.tif|   7.789912427217404|    0|\n",
      "|SeaLake_2111.tif|   8.937356396569768|    0|\n",
      "|SeaLake_2111.tif|    8.64101284717158|    0|\n",
      "| SeaLake_734.tif|   4.697354907272078|    0|\n",
      "| SeaLake_734.tif|   5.828509059939082|    0|\n",
      "| SeaLake_734.tif|   5.531135663737706|    0|\n",
      "| SeaLake_430.tif|   28.09279907438628|    0|\n",
      "| SeaLake_430.tif|  29.309350961954582|    0|\n",
      "| SeaLake_430.tif|  29.204767377349143|    0|\n",
      "|SeaLake_1243.tif|  15.079566832546995|    0|\n",
      "|SeaLake_1243.tif|   16.04900849700115|    0|\n",
      "+----------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/23 20:45:25 WARN CacheManager: Asked to cache already cached data.\n",
      "22/04/23 20:45:25 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+-----+\n",
      "|          origin|isContain_residential|class|\n",
      "+----------------+---------------------+-----+\n",
      "|SeaLake_2847.tif|                    0|    0|\n",
      "|SeaLake_2847.tif|                    0|    0|\n",
      "|SeaLake_2847.tif|                    0|    0|\n",
      "| SeaLake_363.tif|                    0|    0|\n",
      "| SeaLake_363.tif|                    0|    0|\n",
      "| SeaLake_363.tif|                    0|    0|\n",
      "|  River_2019.tif|                    0|    0|\n",
      "|  River_2019.tif|                    0|    0|\n",
      "|  River_2019.tif|                    0|    0|\n",
      "|SeaLake_2111.tif|                    0|    0|\n",
      "|SeaLake_2111.tif|                    0|    0|\n",
      "|SeaLake_2111.tif|                    0|    0|\n",
      "| SeaLake_734.tif|                    0|    0|\n",
      "| SeaLake_734.tif|                    0|    0|\n",
      "| SeaLake_734.tif|                    0|    0|\n",
      "| SeaLake_430.tif|                    0|    0|\n",
      "| SeaLake_430.tif|                    0|    0|\n",
      "| SeaLake_430.tif|                    0|    0|\n",
      "|SeaLake_1243.tif|                    0|    0|\n",
      "|SeaLake_1243.tif|                    0|    0|\n",
      "+----------------+---------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+-----------------------+-----+\n",
      "|          origin|isIntersect_residential|class|\n",
      "+----------------+-----------------------+-----+\n",
      "|SeaLake_2847.tif|                      0|    0|\n",
      "|SeaLake_2847.tif|                      0|    0|\n",
      "|SeaLake_2847.tif|                      0|    0|\n",
      "| SeaLake_363.tif|                      0|    0|\n",
      "| SeaLake_363.tif|                      0|    0|\n",
      "| SeaLake_363.tif|                      0|    0|\n",
      "|  River_2019.tif|                      0|    0|\n",
      "|  River_2019.tif|                      0|    0|\n",
      "|  River_2019.tif|                      0|    0|\n",
      "|SeaLake_2111.tif|                      0|    0|\n",
      "|SeaLake_2111.tif|                      0|    0|\n",
      "|SeaLake_2111.tif|                      0|    0|\n",
      "| SeaLake_734.tif|                      0|    0|\n",
      "| SeaLake_734.tif|                      0|    0|\n",
      "| SeaLake_734.tif|                      0|    0|\n",
      "| SeaLake_430.tif|                      0|    0|\n",
      "| SeaLake_430.tif|                      0|    0|\n",
      "| SeaLake_430.tif|                      0|    0|\n",
      "|SeaLake_1243.tif|                      0|    0|\n",
      "|SeaLake_1243.tif|                      0|    0|\n",
      "+----------------+-----------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+-------------------+-----+\n",
      "|          origin|isTouch_residential|class|\n",
      "+----------------+-------------------+-----+\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "|SeaLake_2847.tif|                  0|    0|\n",
      "| SeaLake_363.tif|                  0|    0|\n",
      "| SeaLake_363.tif|                  0|    0|\n",
      "| SeaLake_363.tif|                  0|    0|\n",
      "|  River_2019.tif|                  0|    0|\n",
      "|  River_2019.tif|                  0|    0|\n",
      "|  River_2019.tif|                  0|    0|\n",
      "|SeaLake_2111.tif|                  0|    0|\n",
      "|SeaLake_2111.tif|                  0|    0|\n",
      "|SeaLake_2111.tif|                  0|    0|\n",
      "| SeaLake_734.tif|                  0|    0|\n",
      "| SeaLake_734.tif|                  0|    0|\n",
      "| SeaLake_734.tif|                  0|    0|\n",
      "| SeaLake_430.tif|                  0|    0|\n",
      "| SeaLake_430.tif|                  0|    0|\n",
      "| SeaLake_430.tif|                  0|    0|\n",
      "|SeaLake_1243.tif|                  0|    0|\n",
      "|SeaLake_1243.tif|                  0|    0|\n",
      "+----------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "residential_points = points.filter((points.attr_key==\"landuse\") & (points.attr_value==\"residential\"))\n",
    "residential_polygons = polygons.filter((polygons.attr_key==\"landuse\") & (polygons.attr_value==\"residential\"))\n",
    "\n",
    "residential_kb = residential_points.union(residential_polygons)\n",
    "residential_kb.persist().count()\n",
    "residential_kb.createOrReplaceTempView(\"residential_kb\")\n",
    "residential_overlap = spark.sql(\"select origin, cast(ST_Overlaps(train.geom, residential_kb.Geometry) as INT) as isOverlaps_residential, class from train, residential_kb\") \n",
    "residential_distance = spark.sql(\"select origin, ST_Distance(train.geom, residential_kb.Geometry) as distance_residential, class from train, residential_kb\")\n",
    "residential_contains = spark.sql(\"select origin, cast(ST_Contains(train.geom, residential_kb.Geometry) as INT) as isContain_residential, class from train, residential_kb\") \n",
    "residential_intersects = spark.sql(\"select origin, cast(ST_Intersects(train.geom, residential_kb.Geometry) as INT) as isIntersect_residential, class from train, residential_kb\") \n",
    "residential_touches = spark.sql(\"select origin, cast(ST_Touches(train.geom, residential_kb.Geometry) as INT) as isTouch_residential, class from train, residential_kb\") \n",
    "\n",
    "residential_overlap.persist().show()\n",
    "residential_distance.persist().show()\n",
    "residential_contains.persist().show()\n",
    "residential_intersects.persist().show()\n",
    "residential_touches.persist().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "152586e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19500"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_11 = residential_overlap.groupby(\"origin\").sum(\"isOverlaps_residential\").alias(\"total_overlap_res\")\n",
    "df_12 = residential_distance.groupby(\"origin\").min(\"distance_residential\").alias(\"distance_res\")\n",
    "df_13 = residential_contains.groupby(\"origin\").sum(\"isContain_residential\").alias(\"total_contain_res\")\n",
    "df_14 = residential_touches.groupby(\"origin\").sum(\"isTouch_residential\").alias(\"total_touch_res\")\n",
    "df_15 = residential_intersects.groupby(\"origin\").sum(\"isIntersect_residential\").alias(\"total_intersect_res\")\n",
    "\n",
    "df_11.persist().count()\n",
    "df_12.persist().count()\n",
    "df_13.persist().count()\n",
    "df_14.persist().count()\n",
    "df_15.persist().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25eeabbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df11 = df_11.toPandas()\n",
    "df12 = df_12.toPandas()\n",
    "df13 = df_13.toPandas()\n",
    "df14 = df_14.toPandas()\n",
    "df15 = df_15.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78260502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df11.merge(df12, on='origin', how=\"inner\")\n",
    "df_final = df13.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df14.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df15.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "\n",
    "df_final.to_csv(\"../data2/euro_sat/residential.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e21458d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+----------+\n",
      "|        id|            Geometry|attr_key|attr_value|\n",
      "+----------+--------------------+--------+----------+\n",
      "|9075746305|POINT (10.2625583...|waterway|     river|\n",
      "|9076734025|POINT (7.3714139 ...|waterway|     river|\n",
      "|9077242944|POINT (6.90884170...|waterway|     river|\n",
      "|9082818118|POINT (7.44840830...|waterway|     river|\n",
      "|9089496939|POINT (7.66552780...|waterway|     river|\n",
      "|9371327551|POINT (6.95857940...|waterway|     river|\n",
      "|8341778501|POINT (23.6479049...|waterway|     river|\n",
      "|8680223148|POINT (10.3615000...|waterway|     river|\n",
      "|8682324704|POINT (7.2427611 ...|waterway|     river|\n",
      "|8682399714|POINT (7.36202780...|waterway|     river|\n",
      "|9350397741|POINT (10.4181462...|waterway|     river|\n",
      "|6990009666|POINT (22.2643725...|waterway|     river|\n",
      "|5196071552|POINT (9.3021887 ...|waterway|     river|\n",
      "|5196071553|POINT (9.3126671 ...|waterway|     river|\n",
      "|5196071554|POINT (9.3253622 ...|waterway|     river|\n",
      "|5196071555|POINT (9.3384795 ...|waterway|     river|\n",
      "|5196071556|POINT (9.3485837 ...|waterway|     river|\n",
      "|5196071557|POINT (9.3515104 ...|waterway|     river|\n",
      "|5196071558|POINT (9.35344870...|waterway|     river|\n",
      "|5196071559|POINT (9.36468530...|waterway|     river|\n",
      "+----------+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+\n",
      "|          origin|    distance_river|class|\n",
      "+----------------+------------------+-----+\n",
      "|SeaLake_2847.tif|23.677458834576523|    0|\n",
      "| SeaLake_363.tif|18.507013383032085|    0|\n",
      "|  River_2019.tif| 6.306190622671269|    0|\n",
      "|SeaLake_2111.tif| 8.245580554305446|    0|\n",
      "| SeaLake_734.tif| 9.620729313007882|    0|\n",
      "| SeaLake_430.tif|23.320396475487858|    0|\n",
      "|SeaLake_1243.tif| 4.646262117243028|    0|\n",
      "| SeaLake_709.tif| 13.57902236628867|    0|\n",
      "|SeaLake_1891.tif|13.162732231200275|    0|\n",
      "|   River_495.tif|14.561146430614574|    0|\n",
      "|SeaLake_2832.tif|17.965024420067888|    0|\n",
      "|SeaLake_2583.tif|17.268646409078166|    0|\n",
      "|SeaLake_2643.tif| 10.65994374997349|    0|\n",
      "|  River_2095.tif|10.594522913017045|    0|\n",
      "|SeaLake_2582.tif|15.252320723743619|    0|\n",
      "|SeaLake_2657.tif| 6.203994300486309|    0|\n",
      "|SeaLake_2834.tif|23.416680714000996|    0|\n",
      "|SeaLake_2317.tif| 4.763208481398384|    0|\n",
      "|  River_2032.tif|14.731214450718245|    0|\n",
      "|SeaLake_1930.tif| 8.073751511827025|    0|\n",
      "+----------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+-----+\n",
      "|          origin|isOverlaps_river|class|\n",
      "+----------------+----------------+-----+\n",
      "|SeaLake_2847.tif|               0|    0|\n",
      "| SeaLake_363.tif|               0|    0|\n",
      "|  River_2019.tif|               0|    0|\n",
      "|SeaLake_2111.tif|               0|    0|\n",
      "| SeaLake_734.tif|               0|    0|\n",
      "| SeaLake_430.tif|               0|    0|\n",
      "|SeaLake_1243.tif|               0|    0|\n",
      "| SeaLake_709.tif|               0|    0|\n",
      "|SeaLake_1891.tif|               0|    0|\n",
      "|   River_495.tif|               0|    0|\n",
      "|SeaLake_2832.tif|               0|    0|\n",
      "|SeaLake_2583.tif|               0|    0|\n",
      "|SeaLake_2643.tif|               0|    0|\n",
      "|  River_2095.tif|               0|    0|\n",
      "|SeaLake_2582.tif|               0|    0|\n",
      "|SeaLake_2657.tif|               0|    0|\n",
      "|SeaLake_2834.tif|               0|    0|\n",
      "|SeaLake_2317.tif|               0|    0|\n",
      "|  River_2032.tif|               0|    0|\n",
      "|SeaLake_1930.tif|               0|    0|\n",
      "+----------------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-----+\n",
      "|          origin|isContain_river|class|\n",
      "+----------------+---------------+-----+\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "| SeaLake_363.tif|              0|    0|\n",
      "|  River_2019.tif|              0|    0|\n",
      "|SeaLake_2111.tif|              0|    0|\n",
      "| SeaLake_734.tif|              0|    0|\n",
      "| SeaLake_430.tif|              0|    0|\n",
      "|SeaLake_1243.tif|              0|    0|\n",
      "| SeaLake_709.tif|              0|    0|\n",
      "|SeaLake_1891.tif|              0|    0|\n",
      "|   River_495.tif|              0|    0|\n",
      "|SeaLake_2832.tif|              0|    0|\n",
      "|SeaLake_2583.tif|              0|    0|\n",
      "|SeaLake_2643.tif|              0|    0|\n",
      "|  River_2095.tif|              0|    0|\n",
      "|SeaLake_2582.tif|              0|    0|\n",
      "|SeaLake_2657.tif|              0|    0|\n",
      "|SeaLake_2834.tif|              0|    0|\n",
      "|SeaLake_2317.tif|              0|    0|\n",
      "|  River_2032.tif|              0|    0|\n",
      "|SeaLake_1930.tif|              0|    0|\n",
      "+----------------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-----+\n",
      "|          origin|isIntersect_river|class|\n",
      "+----------------+-----------------+-----+\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "| SeaLake_363.tif|                0|    0|\n",
      "|  River_2019.tif|                0|    0|\n",
      "|SeaLake_2111.tif|                0|    0|\n",
      "| SeaLake_734.tif|                0|    0|\n",
      "| SeaLake_430.tif|                0|    0|\n",
      "|SeaLake_1243.tif|                0|    0|\n",
      "| SeaLake_709.tif|                0|    0|\n",
      "|SeaLake_1891.tif|                0|    0|\n",
      "|   River_495.tif|                0|    0|\n",
      "|SeaLake_2832.tif|                0|    0|\n",
      "|SeaLake_2583.tif|                0|    0|\n",
      "|SeaLake_2643.tif|                0|    0|\n",
      "|  River_2095.tif|                0|    0|\n",
      "|SeaLake_2582.tif|                0|    0|\n",
      "|SeaLake_2657.tif|                0|    0|\n",
      "|SeaLake_2834.tif|                0|    0|\n",
      "|SeaLake_2317.tif|                0|    0|\n",
      "|  River_2032.tif|                0|    0|\n",
      "|SeaLake_1930.tif|                0|    0|\n",
      "+----------------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                       (0 + 20) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-----+\n",
      "|          origin|isTouch_river|class|\n",
      "+----------------+-------------+-----+\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "| SeaLake_363.tif|            0|    0|\n",
      "|  River_2019.tif|            0|    0|\n",
      "|SeaLake_2111.tif|            0|    0|\n",
      "| SeaLake_734.tif|            0|    0|\n",
      "| SeaLake_430.tif|            0|    0|\n",
      "|SeaLake_1243.tif|            0|    0|\n",
      "| SeaLake_709.tif|            0|    0|\n",
      "|SeaLake_1891.tif|            0|    0|\n",
      "|   River_495.tif|            0|    0|\n",
      "|SeaLake_2832.tif|            0|    0|\n",
      "|SeaLake_2583.tif|            0|    0|\n",
      "|SeaLake_2643.tif|            0|    0|\n",
      "|  River_2095.tif|            0|    0|\n",
      "|SeaLake_2582.tif|            0|    0|\n",
      "|SeaLake_2657.tif|            0|    0|\n",
      "|SeaLake_2834.tif|            0|    0|\n",
      "|SeaLake_2317.tif|            0|    0|\n",
      "|  River_2032.tif|            0|    0|\n",
      "|SeaLake_1930.tif|            0|    0|\n",
      "+----------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 69:================>                                       (6 + 14) / 20]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "river_points = points.filter((points.attr_key==\"waterway\") & (points.attr_value==\"river\"))\n",
    "river_polygons = polygons.filter((polygons.attr_key==\"waterway\") & (polygons.attr_value==\"river\"))\n",
    "river_kb = river_points.union(river_polygons)\n",
    "river_kb.show()\n",
    "river_kb.persist().count()\n",
    "river_kb.createOrReplaceTempView(\"river_kb\")\n",
    "river_overlap = spark.sql(\"select origin, cast(ST_Overlaps(train.geom, river_kb.Geometry) as INT) as isOverlaps_river, class from train, river_kb\") \n",
    "river_distance = spark.sql(\"select origin, ST_Distance(train.geom, river_kb.Geometry) as distance_river, class from train, river_kb\")\n",
    "river_contains = spark.sql(\"select origin, cast(ST_Contains(train.geom, river_kb.Geometry) as INT) as isContain_river, class from train, river_kb\") \n",
    "river_intersects = spark.sql(\"select origin, cast(ST_Intersects(train.geom, river_kb.Geometry) as INT) as isIntersect_river, class from train, river_kb\") \n",
    "river_touches = spark.sql(\"select origin, cast(ST_Touches(train.geom, river_kb.Geometry) as INT) as isTouch_river, class from train, river_kb\") \n",
    "\n",
    "\n",
    "river_distance.persist().show()\n",
    "river_overlap.persist().show()\n",
    "river_contains.persist().show()\n",
    "river_intersects.persist().show()\n",
    "river_touches.persist().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a1f3650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_16 = river_overlap.groupby(\"origin\").sum(\"isOverlaps_river\").alias(\"total_overlap_river\")\n",
    "df_17 = river_distance.groupby(\"origin\").min(\"distance_river\").alias(\"distance_river\")\n",
    "df_18 = river_contains.groupby(\"origin\").sum(\"isContain_river\").alias(\"total_contain_river\")\n",
    "df_19 = river_touches.groupby(\"origin\").sum(\"isTouch_river\").alias(\"total_touch_river\")\n",
    "df_20 = river_intersects.groupby(\"origin\").sum(\"isIntersect_river\").alias(\"total_intersect_river\")\n",
    "\n",
    "df16 = df_16.toPandas()\n",
    "df17 = df_17.toPandas()\n",
    "df18 = df_18.toPandas()\n",
    "df19 = df_19.toPandas()\n",
    "df20 = df_20.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b5cd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df16.merge(df17, on='origin', how=\"inner\")\n",
    "df_final = df18.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df19.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df20.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "\n",
    "df_final.to_csv(\"../data2/euro_sat/river.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f13b04f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/23 20:50:15 WARN CacheManager: Asked to cache already cached data.\n",
      "22/04/23 20:50:15 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+----------+\n",
      "|        id|            Geometry|attr_key|attr_value|\n",
      "+----------+--------------------+--------+----------+\n",
      "|3128843863|POINT (6.86592660...|   water|      lake|\n",
      "|5896261328|POINT (25.7575144...|   water|      lake|\n",
      "|5911625881|POINT (8.4015561 ...|   water|      lake|\n",
      "|9646086926|POINT (11.361081 ...|   water|      lake|\n",
      "|9646086927|POINT (11.3611132...|   water|      lake|\n",
      "|9646086928|POINT (11.3611132...|   water|      lake|\n",
      "|9646086929|POINT (11.3612205...|   water|      lake|\n",
      "|9646086930|POINT (11.3612527...|   water|      lake|\n",
      "|9646086931|POINT (11.3612044...|   water|      lake|\n",
      "|9646086939|POINT (11.3618696...|   water|      lake|\n",
      "|9075644143|POINT (10.3166767...|   water|      lake|\n",
      "|9075725605|POINT (10.2393222...|   water|      lake|\n",
      "|2373058259|POINT (43.4811062...|   water|      lake|\n",
      "|2373058272|POINT (43.4830914...|   water|      lake|\n",
      "|2373058526|POINT (43.4917781...|   water|      lake|\n",
      "|9599461837|POINT (-1.1164557...|   water|      lake|\n",
      "|7736501263|POINT (10.6475059...|   water|      lake|\n",
      "|2349282693|POINT (43.4973467...|   water|      lake|\n",
      "|9076734029|POINT (7.38975110...|   water|      lake|\n",
      "|9104750934|POINT (9.6924468 ...|   water|      lake|\n",
      "+----------+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+------------------+-----+\n",
      "|          origin|    distance_lakes|class|\n",
      "+----------------+------------------+-----+\n",
      "|SeaLake_2847.tif|21.068471428340654|    0|\n",
      "|SeaLake_2847.tif|32.414613986787415|    0|\n",
      "|SeaLake_2847.tif|13.491557228630715|    0|\n",
      "|SeaLake_2847.tif|22.141123277155508|    0|\n",
      "|SeaLake_2847.tif|22.141164329536156|    0|\n",
      "|SeaLake_2847.tif|22.141193178918215|    0|\n",
      "|SeaLake_2847.tif|22.141273972616947|    0|\n",
      "|SeaLake_2847.tif| 22.14124536246277|    0|\n",
      "|SeaLake_2847.tif|22.141188577873738|    0|\n",
      "|SeaLake_2847.tif|22.141603045723148|    0|\n",
      "|SeaLake_2847.tif|23.624799044515033|    0|\n",
      "|SeaLake_2847.tif| 23.61583140951079|    0|\n",
      "|SeaLake_2847.tif|47.048763184342675|    0|\n",
      "|SeaLake_2847.tif| 47.05070429486689|    0|\n",
      "|SeaLake_2847.tif|47.060013531862374|    0|\n",
      "|SeaLake_2847.tif|13.390069494678364|    0|\n",
      "|SeaLake_2847.tif|  17.8150562726905|    0|\n",
      "|SeaLake_2847.tif| 47.06590233922747|    0|\n",
      "|SeaLake_2847.tif| 22.76668134370377|    0|\n",
      "|SeaLake_2847.tif|23.824873984614868|    0|\n",
      "+----------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+---------------+-----+\n",
      "|          origin|isOverlaps_lake|class|\n",
      "+----------------+---------------+-----+\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "+----------------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/23 20:50:16 WARN CacheManager: Asked to cache already cached data.\n",
      "22/04/23 20:50:16 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-----+\n",
      "|          origin|isContain_lakes|class|\n",
      "+----------------+---------------+-----+\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "|SeaLake_2847.tif|              0|    0|\n",
      "+----------------+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-----+\n",
      "|          origin|isIntersect_lakes|class|\n",
      "+----------------+-----------------+-----+\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "|SeaLake_2847.tif|                0|    0|\n",
      "+----------------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 90:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-----+\n",
      "|          origin|isTouch_lakes|class|\n",
      "+----------------+-------------+-----+\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "|SeaLake_2847.tif|            0|    0|\n",
      "+----------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lakes_points = points.filter((points.attr_key==\"water\") & (points.attr_value==\"lake\"))\n",
    "lakes_polygons = polygons.filter((polygons.attr_key==\"water\") & (polygons.attr_value==\"lake\"))\n",
    "lakes_kb = lakes_points.union(lakes_polygons)\n",
    "lakes_kb.persist().show()\n",
    "\n",
    "lakes_kb.createOrReplaceTempView(\"lakes_kb\")\n",
    "lakes_overlap = spark.sql(\"select origin, cast(ST_Overlaps(train.geom, lakes_kb.Geometry) as INT) as isOverlaps_lake, class from train, lakes_kb\") \n",
    "lakes_distance = spark.sql(\"select origin, ST_Distance(train.geom, lakes_kb.Geometry) as distance_lakes, class from train, lakes_kb\")\n",
    "lakes_contains = spark.sql(\"select origin, cast(ST_Contains(train.geom, lakes_kb.Geometry) as INT) as isContain_lakes, class from train, lakes_kb\") \n",
    "lakes_intersects = spark.sql(\"select origin, cast(ST_Intersects(train.geom, lakes_kb.Geometry) as INT) as isIntersect_lakes, class from train, lakes_kb\") \n",
    "lakes_touches = spark.sql(\"select origin, cast(ST_Touches(train.geom, lakes_kb.Geometry) as INT) as isTouch_lakes, class from train, lakes_kb\") \n",
    "\n",
    "lakes_distance.persist().show()\n",
    "lakes_overlap.persist().show()\n",
    "lakes_contains.persist().show()\n",
    "lakes_intersects.persist().show()\n",
    "lakes_touches.persist().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfad1f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_21 = lakes_overlap.groupby(\"origin\").sum(\"isOverlaps_lake\").alias(\"total_overlap_lake\")\n",
    "df_22 = lakes_distance.groupby(\"origin\").min(\"distance_lakes\").alias(\"distance_lake\")\n",
    "df_23 = lakes_contains.groupby(\"origin\").sum(\"isContain_lakes\").alias(\"total_contain_lake\")\n",
    "df_24 = lakes_touches.groupby(\"origin\").sum(\"isTouch_lakes\").alias(\"total_touch_lake\")\n",
    "df_25 = lakes_intersects.groupby(\"origin\").sum(\"isIntersect_lakes\").alias(\"total_intersect_lake\")\n",
    "\n",
    "df21 = df_21.toPandas()\n",
    "df22 = df_22.toPandas()\n",
    "df23 = df_23.toPandas()\n",
    "df24 = df_24.toPandas()\n",
    "df25 = df_25.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea9f481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df21.merge(df22, on='origin', how=\"inner\")\n",
    "df_final = df23.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df24.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df25.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "\n",
    "df_final.to_csv(\"../data2/euro_sat/lakes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f201c97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>sum(isIntersect_lakes)</th>\n",
       "      <th>sum(isTouch_lakes)</th>\n",
       "      <th>sum(isContain_lakes)</th>\n",
       "      <th>sum(isOverlaps_lake)</th>\n",
       "      <th>min(distance_lakes)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Highway_1881.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>Forest_1735.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784</th>\n",
       "      <td>River_2003.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>SeaLake_1166.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3415</th>\n",
       "      <td>Forest_196.tif</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>Forest_734.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4522</th>\n",
       "      <td>Highway_30.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>Highway_1079.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5261</th>\n",
       "      <td>HerbaceousVegetation_1703.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5304</th>\n",
       "      <td>Industrial_109.tif</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             origin  sum(isIntersect_lakes)  \\\n",
       "238                Highway_1881.tif                       1   \n",
       "1685                Forest_1735.tif                       1   \n",
       "2784                 River_2003.tif                       1   \n",
       "2795               SeaLake_1166.tif                       1   \n",
       "3415                 Forest_196.tif                       2   \n",
       "4286                 Forest_734.tif                       1   \n",
       "4522                 Highway_30.tif                       1   \n",
       "4560               Highway_1079.tif                       1   \n",
       "5261  HerbaceousVegetation_1703.tif                       1   \n",
       "5304             Industrial_109.tif                       1   \n",
       "\n",
       "      sum(isTouch_lakes)  sum(isContain_lakes)  sum(isOverlaps_lake)  \\\n",
       "238                    0                     1                     0   \n",
       "1685                   0                     1                     0   \n",
       "2784                   0                     1                     0   \n",
       "2795                   0                     1                     0   \n",
       "3415                   0                     1                     1   \n",
       "4286                   0                     1                     0   \n",
       "4522                   0                     1                     0   \n",
       "4560                   0                     1                     0   \n",
       "5261                   0                     1                     0   \n",
       "5304                   0                     1                     0   \n",
       "\n",
       "      min(distance_lakes)  \n",
       "238                   0.0  \n",
       "1685                  0.0  \n",
       "2784                  0.0  \n",
       "2795                  0.0  \n",
       "3415                  0.0  \n",
       "4286                  0.0  \n",
       "4522                  0.0  \n",
       "4560                  0.0  \n",
       "5261                  0.0  \n",
       "5304                  0.0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final[\"sum(isContain_lakes)\"]>0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9470f269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+----------+\n",
      "|        id|            Geometry|attr_key|attr_value|\n",
      "+----------+--------------------+--------+----------+\n",
      "|3136907158|POINT (9.51959400...| natural| grassland|\n",
      "|1242833995|POINT (-6.6255221...| natural| grassland|\n",
      "|1242860002|POINT (-6.6472476...| natural| grassland|\n",
      "|1243387829|POINT (-3.2368313...| natural| grassland|\n",
      "|2322292063|POINT (36.183979 ...| natural| grassland|\n",
      "|3648585854|POINT (36.1921651...| natural| grassland|\n",
      "|6372458069|POINT (6.16256880...| natural| grassland|\n",
      "|8851596602|POINT (24.0672168...| natural| grassland|\n",
      "|8958429988|POINT (19.0432736...| natural| grassland|\n",
      "|8806651512|POINT (2.35468940...| natural| grassland|\n",
      "|8806651513|POINT (2.3527787 ...| natural| grassland|\n",
      "|7564341484|POINT (40.9019494...| natural| grassland|\n",
      "|8151864033|POINT (10.6729227...| natural| grassland|\n",
      "|8413025726|POINT (9.48333430...| natural| grassland|\n",
      "|4283012817|POINT (17.6103240...| natural| grassland|\n",
      "|3686726898|POINT (-6.9584358...| natural| grassland|\n",
      "|3788597236|POINT (20.6750345...| natural| grassland|\n",
      "|2423360937|POINT (12.7782149...| natural| grassland|\n",
      "|3661735791|POINT (7.68554380...| natural| grassland|\n",
      "|3661735792|POINT (7.6887195 ...| natural| grassland|\n",
      "+----------+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+-----+\n",
      "|          origin|distance_grasslands|class|\n",
      "+----------------+-------------------+-----+\n",
      "|SeaLake_2847.tif| 13.498639333894433|    0|\n",
      "|SeaLake_2847.tif|  7.342695117141778|    0|\n",
      "|SeaLake_2847.tif|  7.254121159977067|    0|\n",
      "|SeaLake_2847.tif| 3.2818968814325866|    0|\n",
      "|SeaLake_2847.tif| 37.968189429321896|    0|\n",
      "|SeaLake_2847.tif|  38.22954992557997|    0|\n",
      "|SeaLake_2847.tif| 23.985222764021355|    0|\n",
      "|SeaLake_2847.tif|   28.2853212039889|    0|\n",
      "|SeaLake_2847.tif|  21.06711692045823|    0|\n",
      "|SeaLake_2847.tif| 3.6206033939888793|    0|\n",
      "|SeaLake_2847.tif|  3.619652212598262|    0|\n",
      "|SeaLake_2847.tif|  45.16686915985807|    0|\n",
      "|SeaLake_2847.tif|   12.0757558387011|    0|\n",
      "|SeaLake_2847.tif| 19.363595707544317|    0|\n",
      "|SeaLake_2847.tif| 27.229262851510104|    0|\n",
      "|SeaLake_2847.tif| 7.0987113894989164|    0|\n",
      "|SeaLake_2847.tif| 22.722567343754044|    0|\n",
      "|SeaLake_2847.tif| 15.326348164829094|    0|\n",
      "|SeaLake_2847.tif| 14.232441045407793|    0|\n",
      "|SeaLake_2847.tif| 14.231994626081638|    0|\n",
      "+----------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 98:=============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+\n",
      "|          origin|isOverlaps_grassland|class|\n",
      "+----------------+--------------------+-----+\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "| SeaLake_363.tif|                   0|    0|\n",
      "|  River_2019.tif|                   0|    0|\n",
      "|SeaLake_2111.tif|                   0|    0|\n",
      "| SeaLake_734.tif|                   0|    0|\n",
      "| SeaLake_430.tif|                   0|    0|\n",
      "|SeaLake_1243.tif|                   0|    0|\n",
      "| SeaLake_709.tif|                   0|    0|\n",
      "|SeaLake_1891.tif|                   0|    0|\n",
      "|   River_495.tif|                   0|    0|\n",
      "|SeaLake_2832.tif|                   0|    0|\n",
      "|SeaLake_2583.tif|                   0|    0|\n",
      "|SeaLake_2643.tif|                   0|    0|\n",
      "|  River_2095.tif|                   0|    0|\n",
      "|SeaLake_2582.tif|                   0|    0|\n",
      "|SeaLake_2657.tif|                   0|    0|\n",
      "|SeaLake_2834.tif|                   0|    0|\n",
      "|SeaLake_2317.tif|                   0|    0|\n",
      "|  River_2032.tif|                   0|    0|\n",
      "|SeaLake_1930.tif|                   0|    0|\n",
      "+----------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 100:============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-----+\n",
      "|          origin|isContain_grasslands|class|\n",
      "+----------------+--------------------+-----+\n",
      "|SeaLake_2847.tif|                   0|    0|\n",
      "| SeaLake_363.tif|                   0|    0|\n",
      "|  River_2019.tif|                   0|    0|\n",
      "|SeaLake_2111.tif|                   0|    0|\n",
      "| SeaLake_734.tif|                   0|    0|\n",
      "| SeaLake_430.tif|                   0|    0|\n",
      "|SeaLake_1243.tif|                   0|    0|\n",
      "| SeaLake_709.tif|                   0|    0|\n",
      "|SeaLake_1891.tif|                   0|    0|\n",
      "|   River_495.tif|                   0|    0|\n",
      "|SeaLake_2832.tif|                   0|    0|\n",
      "|SeaLake_2583.tif|                   0|    0|\n",
      "|SeaLake_2643.tif|                   0|    0|\n",
      "|  River_2095.tif|                   0|    0|\n",
      "|SeaLake_2582.tif|                   0|    0|\n",
      "|SeaLake_2657.tif|                   0|    0|\n",
      "|SeaLake_2834.tif|                   0|    0|\n",
      "|SeaLake_2317.tif|                   0|    0|\n",
      "|  River_2032.tif|                   0|    0|\n",
      "|SeaLake_1930.tif|                   0|    0|\n",
      "+----------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 102:============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+-----+\n",
      "|          origin|isIntersect_grasslands|class|\n",
      "+----------------+----------------------+-----+\n",
      "|SeaLake_2847.tif|                     0|    0|\n",
      "| SeaLake_363.tif|                     0|    0|\n",
      "|  River_2019.tif|                     0|    0|\n",
      "|SeaLake_2111.tif|                     0|    0|\n",
      "| SeaLake_734.tif|                     0|    0|\n",
      "| SeaLake_430.tif|                     0|    0|\n",
      "|SeaLake_1243.tif|                     0|    0|\n",
      "| SeaLake_709.tif|                     0|    0|\n",
      "|SeaLake_1891.tif|                     0|    0|\n",
      "|   River_495.tif|                     0|    0|\n",
      "|SeaLake_2832.tif|                     0|    0|\n",
      "|SeaLake_2583.tif|                     0|    0|\n",
      "|SeaLake_2643.tif|                     0|    0|\n",
      "|  River_2095.tif|                     0|    0|\n",
      "|SeaLake_2582.tif|                     0|    0|\n",
      "|SeaLake_2657.tif|                     0|    0|\n",
      "|SeaLake_2834.tif|                     0|    0|\n",
      "|SeaLake_2317.tif|                     0|    0|\n",
      "|  River_2032.tif|                     0|    0|\n",
      "|SeaLake_1930.tif|                     0|    0|\n",
      "+----------------+----------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+------------------+-----+\n",
      "|          origin|isTouch_grasslands|class|\n",
      "+----------------+------------------+-----+\n",
      "|SeaLake_2847.tif|                 0|    0|\n",
      "| SeaLake_363.tif|                 0|    0|\n",
      "|  River_2019.tif|                 0|    0|\n",
      "|SeaLake_2111.tif|                 0|    0|\n",
      "| SeaLake_734.tif|                 0|    0|\n",
      "| SeaLake_430.tif|                 0|    0|\n",
      "|SeaLake_1243.tif|                 0|    0|\n",
      "| SeaLake_709.tif|                 0|    0|\n",
      "|SeaLake_1891.tif|                 0|    0|\n",
      "|   River_495.tif|                 0|    0|\n",
      "|SeaLake_2832.tif|                 0|    0|\n",
      "|SeaLake_2583.tif|                 0|    0|\n",
      "|SeaLake_2643.tif|                 0|    0|\n",
      "|  River_2095.tif|                 0|    0|\n",
      "|SeaLake_2582.tif|                 0|    0|\n",
      "|SeaLake_2657.tif|                 0|    0|\n",
      "|SeaLake_2834.tif|                 0|    0|\n",
      "|SeaLake_2317.tif|                 0|    0|\n",
      "|  River_2032.tif|                 0|    0|\n",
      "|SeaLake_1930.tif|                 0|    0|\n",
      "+----------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grasslands_points = points.filter((points.attr_key==\"natural\") & (points.attr_value==\"grassland\"))\n",
    "grasslands_polygons = polygons.filter((polygons.attr_key==\"natural\") & (polygons.attr_value==\"grassland\"))\n",
    "grasslands_kb = grasslands_points.union(grasslands_polygons)\n",
    "grasslands_kb.persist().show()\n",
    "\n",
    "grasslands_kb.createOrReplaceTempView(\"grasslands_kb\")\n",
    "grassland_overlap = spark.sql(\"select origin, cast(ST_Overlaps(train.geom, grasslands_kb.Geometry) as INT) as isOverlaps_grassland, class from train, grasslands_kb\") \n",
    "grassland_distance = spark.sql(\"select origin, ST_Distance(train.geom, grasslands_kb.Geometry) as distance_grasslands, class from train,grasslands_kb\")\n",
    "grassland_contains = spark.sql(\"select origin, cast(ST_Contains(train.geom, grasslands_kb.Geometry) as INT) as isContain_grasslands, class from train, grasslands_kb\") \n",
    "grassland_intersects = spark.sql(\"select origin, cast(ST_Intersects(train.geom, grasslands_kb.Geometry) as INT) as isIntersect_grasslands, class from train, grasslands_kb\") \n",
    "grassland_touches = spark.sql(\"select origin, cast(ST_Touches(train.geom, grasslands_kb.Geometry) as INT) as isTouch_grasslands, class from train, grasslands_kb\") \n",
    "\n",
    "grassland_distance.persist().show()\n",
    "grassland_overlap.persist().show()\n",
    "grassland_contains.persist().show()\n",
    "grassland_intersects.persist().show()\n",
    "grassland_touches.persist().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f3cb757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_26 = grassland_overlap.groupby(\"origin\").sum(\"isOverlaps_grassland\").alias(\"total_overlap_lake\")\n",
    "df_27 = grassland_distance.groupby(\"origin\").min(\"distance_grasslands\").alias(\"distance_lake\")\n",
    "df_28 = grassland_contains.groupby(\"origin\").sum(\"isContain_grasslands\").alias(\"total_contain_lake\")\n",
    "df_29 = grassland_touches.groupby(\"origin\").sum(\"isTouch_grasslands\").alias(\"total_touch_lake\")\n",
    "df_30 = grassland_intersects.groupby(\"origin\").sum(\"isIntersect_grasslands\").alias(\"total_intersect_lake\")\n",
    "\n",
    "df26 = df_26.toPandas()\n",
    "df27 = df_27.toPandas()\n",
    "df28 = df_28.toPandas()\n",
    "df29 = df_29.toPandas()\n",
    "df30 = df_30.toPandas()\n",
    "\n",
    "df_final = df26.merge(df27, on='origin', how=\"inner\")\n",
    "df_final = df28.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df29.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df30.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "\n",
    "df_final.to_csv(\"../data2/euro_sat/grasslands.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31915db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/27 17:55:20 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------+---------------+\n",
      "|       id|            Geometry|attr_key|     attr_value|\n",
      "+---------+--------------------+--------+---------------+\n",
      "|340497091|POINT (24.9726741...| highway|       crossing|\n",
      "|340497126|POINT (24.9696257...| highway|       bus_stop|\n",
      "|340497179|POINT (-3.7021749...| highway|       crossing|\n",
      "|340498257|POINT (10.2724356...| highway| turning_circle|\n",
      "|340498741|POINT (-1.7144941...| highway|           stop|\n",
      "|340499204|POINT (10.6091171...| highway| turning_circle|\n",
      "|340499528|POINT (6.41288860...| highway|       crossing|\n",
      "|340500976|POINT (-0.4695351...| highway|traffic_signals|\n",
      "|340505216|POINT (6.0935505 ...| highway|       crossing|\n",
      "|340505304|POINT (6.09609780...| highway| turning_circle|\n",
      "|340505336|POINT (6.0932594 ...| highway| turning_circle|\n",
      "|340505410|POINT (6.0921151 ...| highway| turning_circle|\n",
      "|340505437|POINT (6.09917080...| highway|       crossing|\n",
      "|340505468|POINT (6.1072534 ...| highway| turning_circle|\n",
      "|340505489|POINT (6.10970460...| highway| turning_circle|\n",
      "|340512460|POINT (6.1121822 ...| highway|       crossing|\n",
      "|340514230|POINT (-2.5758778...| highway|  passing_place|\n",
      "|340514232|POINT (-2.5750322...| highway|  passing_place|\n",
      "|340515246|POINT (9.0598449 ...| highway| turning_circle|\n",
      "|340515257|POINT (12.0971033...| highway| turning_circle|\n",
      "+---------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"refresh progress\"                                          java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/04/27 18:36:27 WARN TransportChannelHandler: Exception in connection from /10.218.108.131:32816\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat java.util.regex.Pattern.compile(Pattern.java:1693)\n",
      "\tat java.util.regex.Pattern.<init>(Pattern.java:1352)\n",
      "\tat java.util.regex.Pattern.compile(Pattern.java:1028)\n",
      "\tat org.apache.spark.network.util.JavaUtils.timeStringAs(JavaUtils.java:232)\n",
      "\tat org.apache.spark.network.util.JavaUtils.timeStringAsSec(JavaUtils.java:269)\n",
      "\tat org.apache.spark.util.Utils$.timeStringAsSeconds(Utils.scala:1179)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$.apply(RpcTimeout.scala:131)\n",
      "\tat org.apache.spark.util.RpcUtils$.askRpcTimeout(RpcUtils.scala:51)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.<init>(RpcEndpointRef.scala:35)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.<init>(NettyRpcEnv.scala:534)\n",
      "\tat org.apache.spark.rpc.netty.RequestMessage$.apply(NettyRpcEnv.scala:641)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcHandler.internalReceive(NettyRpcEnv.scala:698)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:683)\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:161)\n",
      "\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:109)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n",
      "\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n",
      "\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n",
      "22/04/27 18:36:27 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 2303675 ms exceeds timeout 1000000 ms\n",
      "22/04/27 18:36:27 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$851/546800689.get$Lambda(Unknown Source)\n",
      "\tat java.lang.invoke.LambdaForm$DMH/2047526627.invokeStatic_L_L(LambdaForm$DMH)\n",
      "\tat java.lang.invoke.LambdaForm$MH/909295153.linkToTargetMethod(LambdaForm$MH)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:186)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$747/1580652877.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:180)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:77)\n",
      "22/04/27 18:36:27 WARN DefaultChannelPipeline: An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception.\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/04/27 18:36:27 WARN TransportChannelHandler: Exception in connection from /10.218.106.130:35938\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/04/27 18:36:27 WARN TransportChannelHandler: Exception in connection from en4119508l.cidse.dhcp.asu.edu/10.218.106.130:7077\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/04/27 18:36:27 ERROR Inbox: An error happened while processing message in the inbox for CoarseGrainedScheduler\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/04/27 18:36:27 ERROR Inbox: An error happened while processing message in the inbox for HeartbeatReceiver\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/04/27 18:36:27 WARN TransportChannelHandler: Exception in connection from /10.218.106.130:40054\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/04/27 18:36:27 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 2303675 ms exceeds timeout 1000000 ms\n",
      "Exception in thread \"dispatcher-event-loop-24\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/04/27 18:36:27 WARN StandaloneAppClient$ClientEndpoint: Could not connect to EN4119508L.cidse.dhcp.asu.edu:7077: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "22/04/27 18:36:27 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$851/546800689.get$Lambda(Unknown Source)\n",
      "\tat java.lang.invoke.LambdaForm$DMH/2047526627.invokeStatic_L_L(LambdaForm$DMH)\n",
      "\tat java.lang.invoke.LambdaForm$MH/909295153.linkToTargetMethod(LambdaForm$MH)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:186)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$747/1580652877.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:180)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:77)\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$851/546800689.get$Lambda(Unknown Source)\n",
      "\tat java.lang.invoke.LambdaForm$DMH/2047526627.invokeStatic_L_L(LambdaForm$DMH)\n",
      "\tat java.lang.invoke.LambdaForm$MH/909295153.linkToTargetMethod(LambdaForm$MH)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:186)\n",
      "\tat org.apache.spark.ContextCleaner$$Lambda$747/1580652877.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:180)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:77)\n",
      "22/04/27 18:36:27 WARN NettyRpcEnv: Ignored message: true\n",
      "22/04/27 18:36:27 ERROR TaskSchedulerImpl: Lost executor 0 on 10.218.108.131: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/27 18:36:27 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=8666012102959502266,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]]] to /10.218.108.131:32816; closing connection\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/27 18:36:27 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=6329856509659231499,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]]] to /10.218.106.130:40054; closing connection\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/27 18:36:27 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=7112390455456661523,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]]] to /10.218.108.131:32816; closing connection\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/27 18:36:27 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=5810430864542290046,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]]] to /10.218.106.130:40054; closing connection\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/27 18:36:27 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=8668150739906692432,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]]] to /10.218.108.131:32816; closing connection\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/27 18:36:27 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=4765588769829247597,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]]] to /10.218.106.130:40054; closing connection\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/27 18:36:27 ERROR TaskSchedulerImpl: Lost executor 1 on 10.218.106.130: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/04/27 18:36:27 ERROR TransportClient: Failed to send RPC RPC 5918017518677746602 to en4119508l.cidse.dhcp.asu.edu/10.218.106.130:7077: java.nio.channels.ClosedChannelException\n",
      "java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/27 18:36:27 ERROR Utils: Uncaught exception in thread kill-executor-thread\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.killExecutors(CoarseGrainedSchedulerBackend.scala:843)\n",
      "\tat org.apache.spark.SparkContext.killAndReplaceExecutor(SparkContext.scala:1811)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.$anonfun$run$2(HeartbeatReceiver.scala:214)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.run(HeartbeatReceiver.scala:211)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.io.IOException: Failed to send RPC RPC 5918017518677746602 to en4119508l.cidse.dhcp.asu.edu/10.218.106.130:7077: java.nio.channels.ClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:363)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:340)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: java.nio.channels.ClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n",
      "\t... 12 more\n",
      "22/04/27 18:36:27 WARN StandaloneSchedulerBackend: Executor to kill 0 does not exist!\n",
      "22/04/27 18:36:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:150)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.send(NettyRpcEnv.scala:193)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.send(NettyRpcEnv.scala:564)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:600)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.executorRemoved(StandaloneSchedulerBackend.scala:178)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:183)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/27 18:36:27 ERROR Utils: Uncaught exception in thread kill-executor-thread\n",
      "org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:150)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.send(NettyRpcEnv.scala:193)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.send(NettyRpcEnv.scala:564)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.$anonfun$run$2(HeartbeatReceiver.scala:225)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\n",
      "\tat org.apache.spark.HeartbeatReceiver$$anon$2.run(HeartbeatReceiver.scala:211)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java."
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2295.showString.\n: java.util.concurrent.ExecutionException: org.apache.spark.util.SparkFatalException\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doExecute(BroadcastNestedLoopJoinExec.scala:350)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:92)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.util.SparkFatalException\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:168)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49063/3694629738.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhighway_touches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select origin, cast(ST_Touches(train.geom, highway_kb.Geometry) as INT) as isTouch_highways, class from train, highway_kb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mhighway_distance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mhighway_overlap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mhighway_contains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hdd2/shantanuCodeData/lib/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/knowledgebase-ezGPqgFo/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hdd2/shantanuCodeData/lib/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/knowledgebase-ezGPqgFo/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2295.showString.\n: java.util.concurrent.ExecutionException: org.apache.spark.util.SparkFatalException\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:194)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeBroadcast$1(SparkPlan.scala:193)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:189)\n\tat org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec.doExecute(BroadcastNestedLoopJoinExec.scala:350)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:92)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.util.SparkFatalException\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:168)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/04/27 18:36:27 WARN StandaloneAppClient$ClientEndpoint: Connection to EN4119508L.cidse.dhcp.asu.edu:7077 failed; waiting for master to reconnect...\n"
     ]
    }
   ],
   "source": [
    "highway_points = points.filter(points.attr_key==\"highway\")\n",
    "highway_polygons = polygons.filter(polygons.attr_key==\"highway\")\n",
    "highway_kb = highway_points.union(highway_polygons)\n",
    "highway_kb.persist().show()\n",
    "geo_train_df.createOrReplaceTempView(\"train\")\n",
    "\n",
    "\n",
    "highway_kb.createOrReplaceTempView(\"highway_kb\")\n",
    "highway_overlap = spark.sql(\"select origin, cast(ST_Overlaps(train.geom, highway_kb.Geometry) as INT) as isOverlaps_highway, class from train, highway_kb\") \n",
    "highway_distance = spark.sql(\"select origin, ST_Distance(train.geom, highway_kb.Geometry) as distance_highway, class from train,highway_kb\")\n",
    "highway_contains = spark.sql(\"select origin, cast(ST_Contains(train.geom, highway_kb.Geometry) as INT) as isContain_highways, class from train, highway_kb\") \n",
    "highway_intersects = spark.sql(\"select origin, cast(ST_Intersects(train.geom, highway_kb.Geometry) as INT) as isIntersect_highways, class from train, highway_kb\") \n",
    "highway_touches = spark.sql(\"select origin, cast(ST_Touches(train.geom, highway_kb.Geometry) as INT) as isTouch_highways, class from train, highway_kb\") \n",
    "\n",
    "highway_distance.persist().show()\n",
    "highway_overlap.persist().show()\n",
    "highway_contains.persist().show()\n",
    "highway_intersects.persist().show()\n",
    "highway_touches.persist().show()\n",
    "\n",
    "df_31 = highway_overlap.groupby(\"origin\").sum(\"isOverlaps_highway\").alias(\"total_overlap_lake\")\n",
    "df_32 = highway_distance.groupby(\"origin\").min(\"distance_highway\").alias(\"distance_lake\")\n",
    "df_33 = highway_contains.groupby(\"origin\").sum(\"isContain_highways\").alias(\"total_contain_lake\")\n",
    "df_34 = highway_touches.groupby(\"origin\").sum(\"isTouch_highways\").alias(\"total_touch_lake\")\n",
    "df_35 = highway_intersects.groupby(\"origin\").sum(\"isIntersect_highways\").alias(\"total_intersect_lake\")\n",
    "\n",
    "df31 = df_31.toPandas()\n",
    "df32 = df_32.toPandas()\n",
    "df33 = df_33.toPandas()\n",
    "df34 = df_34.toPandas()\n",
    "df35 = df_35.toPandas()\n",
    "\n",
    "df_final = df31.merge(df32, on='origin', how=\"inner\")\n",
    "df_final = df33.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df34.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "df_final = df35.merge(df_final, on=\"origin\", how=\"inner\")\n",
    "\n",
    "df_final.to_csv(\"../data2/euro_sat/highways.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cbd81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache-sedona",
   "language": "python",
   "name": "apache-sedona"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
